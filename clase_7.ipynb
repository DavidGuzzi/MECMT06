{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadstat as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lfp</th>\n",
       "      <th>whrs</th>\n",
       "      <th>kl6</th>\n",
       "      <th>k618</th>\n",
       "      <th>wa</th>\n",
       "      <th>we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1610</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1656</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1980</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>456</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1568</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lfp  whrs  kl6  k618  wa  we\n",
       "0    1  1610    1     0  32  12\n",
       "1    1  1656    0     2  30  12\n",
       "2    1  1980    1     3  35  12\n",
       "3    1   456    0     3  34  12\n",
       "4    1  1568    1     2  31  14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"C:\\Users\\HP\\OneDrive\\Escritorio\\David Guzzi\\DiTella\\MEC\\Materias\\2024 3T\\[MT06] Microeconometría I\\Clases\\Stata\\Archivos Stata-20250107\\laborsub.dta\"\n",
    "\n",
    "df, meta = st.read_dta(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de parámetros estimados: 6\n",
      "Número de nombres de parámetros: 6\n",
      "   Parameter     Estimate\n",
      "0  Intercept   591.599993\n",
      "1        kl6  -828.418990\n",
      "2       k618  -140.121376\n",
      "3         wa   -25.018650\n",
      "4         we   103.628917\n",
      "5      sigma -1310.119449\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# Preparar los datos\n",
    "X = sm.add_constant(df[[\"kl6\", \"k618\", \"wa\", \"we\"]])  # Datos para las variables independientes\n",
    "y = df[\"whrs\"]  # Variable dependiente\n",
    "\n",
    "# Definir función de verosimilitud\n",
    "def log_likelihood(params, X, y):\n",
    "    beta = params[:-1]\n",
    "    sigma = np.abs(params[-1])  # Restricción de sigma > 0\n",
    "    \n",
    "    mu = X @ beta\n",
    "    \n",
    "    # Usar un epsilon pequeño para evitar logaritmos de cero\n",
    "    epsilon = 1e-10\n",
    "    pdf_values = norm.pdf(y, mu, sigma)\n",
    "    cdf_values = norm.cdf(0, mu, sigma)\n",
    "    \n",
    "    # Asegurar que los valores no sean cero en el logaritmo\n",
    "    log_pdf = np.log(np.maximum(pdf_values, epsilon))\n",
    "    log_cdf = np.log(np.maximum(cdf_values, epsilon))\n",
    "    \n",
    "    ll = np.sum((y > 0) * log_pdf) + np.sum((y == 0) * log_cdf)\n",
    "    \n",
    "    return -np.sum(ll)  # Se minimiza la log-verosimilitud negativa\n",
    "\n",
    "# Estimación MLE\n",
    "init_params = np.ones(X.shape[1] + 1)  # Inicialización de parámetros\n",
    "res = minimize(log_likelihood, init_params, args=(X, y), method=\"L-BFGS-B\")\n",
    "params = res.x\n",
    "\n",
    "# Verificar la cantidad de parámetros y ajustarlos\n",
    "param_names = [\"Intercept\"] + list(df[[\"kl6\", \"k618\", \"wa\", \"we\"]].columns) + [\"sigma\"]\n",
    "\n",
    "# Comprobar la longitud de ambos\n",
    "print(\"Número de parámetros estimados:\", len(params))\n",
    "print(\"Número de nombres de parámetros:\", len(param_names))\n",
    "\n",
    "# Si el número de parámetros es correcto, crear el DataFrame de resultados\n",
    "if len(params) == len(param_names):\n",
    "    results_df = pd.DataFrame({\"Parameter\": param_names, \"Estimate\": params})\n",
    "    print(results_df)\n",
    "else:\n",
    "    # Si hay un desajuste, ajustamos la cantidad de parámetros\n",
    "    print(f\"Desajuste en la longitud de parámetros: {len(params)} vs {len(param_names)}.\")\n",
    "    if len(params) > len(param_names):\n",
    "        param_names += [\"extra_param_\" + str(i) for i in range(len(param_names), len(params))]\n",
    "    elif len(params) < len(param_names):\n",
    "        param_names = param_names[:len(params)]  # Recortamos si es necesario\n",
    "    \n",
    "    # Crear DataFrame ajustado\n",
    "    results_df = pd.DataFrame({\"Parameter\": param_names, \"Estimate\": params})\n",
    "    print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de parámetros estimados: 6\n",
      "Número de nombres de parámetros: 6\n",
      "   Parameter     Estimate\n",
      "0  Intercept   244.967224\n",
      "1        kl6  -258.603046\n",
      "2       k618   -63.641982\n",
      "3         wa    -2.871990\n",
      "4         we     5.887783\n",
      "5      sigma -1528.873225\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# Preparar los datos\n",
    "df2 = df[df['whrs'] > 0]  # Filtrar solo observaciones no censuradas\n",
    "X = sm.add_constant(df2[[\"kl6\", \"k618\", \"wa\", \"we\"]])  # Datos para las variables independientes\n",
    "y = df2[\"whrs\"]  # Variable dependiente\n",
    "\n",
    "# Definir función de verosimilitud con censura en 0\n",
    "def log_likelihood(params, X, y):\n",
    "    beta = params[:-1]\n",
    "    sigma = np.abs(params[-1])  # Restricción de sigma > 0\n",
    "    \n",
    "    mu = X @ beta\n",
    "    \n",
    "    # Usar un epsilon pequeño para evitar logaritmos de cero\n",
    "    epsilon = 1e-100\n",
    "    pdf_values = norm.pdf(y, mu, sigma)\n",
    "    cdf_values = norm.cdf(0, mu, sigma)\n",
    "    \n",
    "    # Log-verosimilitud para observaciones no censuradas (y > 0)\n",
    "    log_pdf = np.log(np.maximum(pdf_values, epsilon))\n",
    "    \n",
    "    # Log-verosimilitud para observaciones censuradas en 0 (y == 0)\n",
    "    log_cdf = np.log(np.maximum(cdf_values, epsilon))\n",
    "    \n",
    "    # La log-verosimilitud total\n",
    "    ll = log_pdf - log_cdf\n",
    "    \n",
    "    return -np.sum(ll)  # Se minimiza la log-verosimilitud negativa\n",
    "\n",
    "# Estimación MLE\n",
    "init_params = np.ones(X.shape[1] + 1)  # Inicialización de parámetros\n",
    "res = minimize(log_likelihood, init_params, args=(X, y), method=\"L-BFGS-B\")\n",
    "params = res.x\n",
    "\n",
    "# Verificar la cantidad de parámetros y ajustarlos\n",
    "param_names = [\"Intercept\"] + list(df[[\"kl6\", \"k618\", \"wa\", \"we\"]].columns) + [\"sigma\"]\n",
    "\n",
    "# Comprobar la longitud de ambos\n",
    "print(\"Número de parámetros estimados:\", len(params))\n",
    "print(\"Número de nombres de parámetros:\", len(param_names))\n",
    "\n",
    "# Si el número de parámetros es correcto, crear el DataFrame de resultados\n",
    "if len(params) == len(param_names):\n",
    "    results_df = pd.DataFrame({\"Parameter\": param_names, \"Estimate\": params})\n",
    "    print(results_df)\n",
    "else:\n",
    "    # Si hay un desajuste, ajustamos la cantidad de parámetros\n",
    "    print(f\"Desajuste en la longitud de parámetros: {len(params)} vs {len(param_names)}.\")\n",
    "    if len(params) > len(param_names):\n",
    "        param_names += [\"extra_param_\" + str(i) for i in range(len(param_names), len(params))]\n",
    "    elif len(params) < len(param_names):\n",
    "        param_names = param_names[:len(params)]  # Recortamos si es necesario\n",
    "    \n",
    "    # Crear DataFrame ajustado\n",
    "    results_df = pd.DataFrame({\"Parameter\": param_names, \"Estimate\": params})\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de parámetros estimados: 6\n",
      "Número de nombres de parámetros: 6\n",
      "   Parameter     Estimate\n",
      "0  Intercept   587.433584\n",
      "1        kl6  -827.538027\n",
      "2       k618  -139.951544\n",
      "3         wa   -24.964737\n",
      "4         we   103.738070\n",
      "5      sigma  1309.832381\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# Preparar los datos\n",
    "X = sm.add_constant(df[[\"kl6\", \"k618\", \"wa\", \"we\"]])  # Datos para las variables independientes\n",
    "y = df[\"whrs\"]  # Variable dependiente\n",
    "\n",
    "# Definir función de verosimilitud\n",
    "def log_likelihood(params, X, y):\n",
    "    beta = params[:-1]\n",
    "    sigma = np.abs(params[-1])  # Restricción de sigma > 0\n",
    "    \n",
    "    mu = X @ beta\n",
    "    \n",
    "    # Usar un epsilon pequeño para evitar logaritmos de cero\n",
    "    epsilon = 1e-100\n",
    "    # pdf_values = norm.pdf(y, mu, sigma)\n",
    "    # cdf_values = norm.cdf(0, mu, sigma)\n",
    "    \n",
    "    # Asegurar que los valores no sean cero en el logaritmo\n",
    "    log_pdf = -0.5 * np.log(2 * np.pi) - 0.5 * np.log(sigma**2) - ((y - mu) ** 2) / (2 * sigma**2)\n",
    "    log_cdf = np.log(np.maximum(1 - norm.cdf(mu / sigma), epsilon))\n",
    "    \n",
    "    ll = np.sum((y > 0) * log_pdf) + np.sum((y == 0) * log_cdf)\n",
    "    \n",
    "    return -np.sum(ll)  # Se minimiza la log-verosimilitud negativa\n",
    "\n",
    "# Estimación MLE\n",
    "init_params = np.ones(X.shape[1] + 1)  # Inicialización de parámetros\n",
    "res = minimize(log_likelihood, init_params, args=(X, y), method=\"L-BFGS-B\")\n",
    "params = res.x\n",
    "\n",
    "# Verificar la cantidad de parámetros y ajustarlos\n",
    "param_names = [\"Intercept\"] + list(df[[\"kl6\", \"k618\", \"wa\", \"we\"]].columns) + [\"sigma\"]\n",
    "\n",
    "# Comprobar la longitud de ambos\n",
    "print(\"Número de parámetros estimados:\", len(params))\n",
    "print(\"Número de nombres de parámetros:\", len(param_names))\n",
    "\n",
    "# Si el número de parámetros es correcto, crear el DataFrame de resultados\n",
    "if len(params) == len(param_names):\n",
    "    results_df = pd.DataFrame({\"Parameter\": param_names, \"Estimate\": params})\n",
    "    print(results_df)\n",
    "else:\n",
    "    # Si hay un desajuste, ajustamos la cantidad de parámetros\n",
    "    print(f\"Desajuste en la longitud de parámetros: {len(params)} vs {len(param_names)}.\")\n",
    "    if len(params) > len(param_names):\n",
    "        param_names += [\"extra_param_\" + str(i) for i in range(len(param_names), len(params))]\n",
    "    elif len(params) < len(param_names):\n",
    "        param_names = param_names[:len(params)]  # Recortamos si es necesario\n",
    "    \n",
    "    # Crear DataFrame ajustado\n",
    "    results_df = pd.DataFrame({\"Parameter\": param_names, \"Estimate\": params})\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de parámetros estimados: 6\n",
      "Número de nombres de parámetros: 6\n",
      "   Parameter     Estimate\n",
      "0  Intercept    -5.351723\n",
      "1        kl6  -292.050044\n",
      "2       k618   -57.848851\n",
      "3         wa    -8.431854\n",
      "4         we    32.351200\n",
      "5      sigma  1235.111405\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# Preparar los datos\n",
    "X = sm.add_constant(df[[\"kl6\", \"k618\", \"wa\", \"we\"]])  # Datos para las variables independientes\n",
    "y = df[\"whrs\"]  # Variable dependiente\n",
    "\n",
    "# Definir función de verosimilitud\n",
    "def log_likelihood(params, X, y):\n",
    "    beta = params[:-1]\n",
    "    sigma = np.abs(params[-1])  # Restricción de sigma > 0\n",
    "    \n",
    "    mu = X @ beta\n",
    "    \n",
    "    # Usar un epsilon pequeño para evitar logaritmos de cero\n",
    "    epsilon = 1e-100\n",
    "    # pdf_values = norm.pdf(y, mu, sigma)\n",
    "    # cdf_values = norm.cdf(0, mu, sigma)\n",
    "    \n",
    "    # Asegurar que los valores no sean cero en el logaritmo\n",
    "    log_pdf = -0.5 * np.log(2 * np.pi) - 0.5 * np.log(sigma**2) - ((y - mu) ** 2) / (2 * sigma**2)\n",
    "    log_cdf = np.log(np.maximum(1 - norm.cdf(mu / sigma), epsilon))\n",
    "    \n",
    "    ll = log_pdf + log_cdf\n",
    "    \n",
    "    return -np.sum(ll)  # Se minimiza la log-verosimilitud negativa\n",
    "\n",
    "# Estimación MLE\n",
    "init_params = np.ones(X.shape[1] + 1)  # Inicialización de parámetros\n",
    "res = minimize(log_likelihood, init_params, args=(X, y), method=\"L-BFGS-B\")\n",
    "params = res.x\n",
    "\n",
    "# Verificar la cantidad de parámetros y ajustarlos\n",
    "param_names = [\"Intercept\"] + list(df[[\"kl6\", \"k618\", \"wa\", \"we\"]].columns) + [\"sigma\"]\n",
    "\n",
    "# Comprobar la longitud de ambos\n",
    "print(\"Número de parámetros estimados:\", len(params))\n",
    "print(\"Número de nombres de parámetros:\", len(param_names))\n",
    "\n",
    "# Si el número de parámetros es correcto, crear el DataFrame de resultados\n",
    "if len(params) == len(param_names):\n",
    "    results_df = pd.DataFrame({\"Parameter\": param_names, \"Estimate\": params})\n",
    "    print(results_df)\n",
    "else:\n",
    "    # Si hay un desajuste, ajustamos la cantidad de parámetros\n",
    "    print(f\"Desajuste en la longitud de parámetros: {len(params)} vs {len(param_names)}.\")\n",
    "    if len(params) > len(param_names):\n",
    "        param_names += [\"extra_param_\" + str(i) for i in range(len(param_names), len(params))]\n",
    "    elif len(params) < len(param_names):\n",
    "        param_names = param_names[:len(params)]  # Recortamos si es necesario\n",
    "    \n",
    "    # Crear DataFrame ajustado\n",
    "    results_df = pd.DataFrame({\"Parameter\": param_names, \"Estimate\": params})\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Parameter     Estimate\n",
      "0  Intercept  1584.614114\n",
      "1        kl6  -802.626576\n",
      "2       k618  -172.951631\n",
      "3         wa    -8.781402\n",
      "4         we    16.496883\n",
      "5      sigma   984.141866\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "True\n",
      "81\n",
      "1200.915753776213\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# Preparar los datos\n",
    "df2 = df[df['whrs'] > 0]  # Filtrar solo observaciones no censuradas\n",
    "X = sm.add_constant(df2[[\"kl6\", \"k618\", \"wa\", \"we\"]])  # Variables independientes\n",
    "y = df2[\"whrs\"]  # Variable dependiente\n",
    "\n",
    "# Definir función de log-verosimilitud para regresión truncada\n",
    "def log_likelihood(params, X, y):\n",
    "    beta = params[:-1]  # Coeficientes de regresión\n",
    "    sigma = np.abs(params[-1])  # Restricción de sigma > 0\n",
    "    \n",
    "    mu = X @ beta  # Media condicional\n",
    "    \n",
    "    # Densidad normal en los valores observados\n",
    "    log_pdf = -0.5 * np.log(2 * np.pi) - 0.5 * np.log(sigma**2) - ((y - mu) ** 2) / (2 * sigma**2)\n",
    "    \n",
    "    # CDF evaluada en el punto de truncamiento (0)\n",
    "    log_cdf = np.log(np.maximum(1 - norm.cdf(0, loc=mu, scale=sigma), 1e-100))\n",
    "    \n",
    "    # Log-verosimilitud total\n",
    "    ll = log_pdf - log_cdf\n",
    "    \n",
    "    return -np.sum(ll)  # Se minimiza la log-verosimilitud negativa\n",
    "\n",
    "# Estimación MLE\n",
    "init_params = np.ones(X.shape[1] + 1)  # Inicialización de parámetros\n",
    "res = minimize(log_likelihood, init_params, args=(X, y), method=\"L-BFGS-B\")\n",
    "\n",
    "# Extraer los parámetros estimados\n",
    "params = res.x\n",
    "param_names = [\"Intercept\"] + list(df[[\"kl6\", \"k618\", \"wa\", \"we\"]].columns) + [\"sigma\"]\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "results_df = pd.DataFrame({\"Parameter\": param_names, \"Estimate\": params})\n",
    "print(results_df)\n",
    "print(res.message)  # Mensaje del optimizador\n",
    "print(res.success)  # Indica si la optimización fue exitosa\n",
    "print(res.nit)      # Número de iteraciones realizadas\n",
    "print(res.fun)      # Valor final de la función objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud de params: 6\n",
      "   Parameter     Estimate\n",
      "0  Intercept   187.768790\n",
      "1        kl6  -322.206536\n",
      "2       k618   -83.909154\n",
      "3         wa    -7.104857\n",
      "4         we     9.318657\n",
      "5      sigma  1633.607870\n",
      "Convergencia: False\n",
      "Iteraciones: 3\n",
      "Valor de la función de verosimilitud: 1211.0044012131093\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# Preparar los datos\n",
    "df2 = df[df['whrs'] > 0]  # Filtrar solo observaciones no censuradas\n",
    "X = sm.add_constant(df2[[\"kl6\", \"k618\", \"wa\", \"we\"]])  # Variables independientes\n",
    "y = df2[\"whrs\"]  # Variable dependiente\n",
    "\n",
    "# Normalización de X para mejorar estabilidad numérica\n",
    "X_mean = X.mean()\n",
    "X_std = X.std()\n",
    "X_std = X_std.replace(0, 1)  # Evitar divisiones por cero en columnas constantes\n",
    "X = (X - X_mean) / X_std\n",
    "\n",
    "# Definir función de log-verosimilitud para regresión truncada\n",
    "def log_likelihood(params, X, y):\n",
    "    beta = params[:-1]  # Coeficientes de regresión\n",
    "    sigma = np.abs(params[-1])  # Restricción de sigma > 0\n",
    "    \n",
    "    mu = X @ beta  # Media condicional\n",
    "    \n",
    "    # Densidad normal en los valores observados\n",
    "    log_pdf = -0.5 * np.log(2 * np.pi) - np.log(sigma) - ((y - mu) ** 2) / (2 * sigma**2)\n",
    "    \n",
    "    # CDF evaluada en el punto de truncamiento (0)\n",
    "    log_cdf = np.log(np.maximum(1 - norm.cdf(0, loc=mu, scale=sigma), 1e-100))\n",
    "    \n",
    "    # Log-verosimilitud total\n",
    "    ll = log_pdf - log_cdf\n",
    "    \n",
    "    return -np.sum(ll)  # Se minimiza la log-verosimilitud negativa\n",
    "\n",
    "# Función de gradiente (primeras derivadas)\n",
    "def gradient(params, X, y):\n",
    "    beta = params[:-1]\n",
    "    sigma = np.abs(params[-1])\n",
    "    \n",
    "    mu = X @ beta\n",
    "    z = (y - mu) / sigma\n",
    "    phi_z = norm.pdf(z)\n",
    "    Phi_z = np.maximum(1 - norm.cdf(0, loc=mu, scale=sigma), 1e-100)\n",
    "\n",
    "    # Gradiente respecto a beta\n",
    "    grad_beta = X.T @ ((y - mu) / sigma**2 + phi_z / (sigma * Phi_z))\n",
    "    \n",
    "    # Gradiente respecto a sigma (corrigiendo posible inestabilidad numérica)\n",
    "    grad_sigma = np.sum(((y - mu)**2 / sigma**3 - 1 / sigma) + phi_z * (y - mu) / (sigma**2 * Phi_z))\n",
    "    \n",
    "    return -np.append(grad_beta, grad_sigma)  # Minimizar, por eso el signo negativo\n",
    "\n",
    "# Inicialización con OLS para mejor convergencia\n",
    "ols_params = np.append(np.linalg.lstsq(X, y, rcond=None)[0], np.std(y))\n",
    "\n",
    "# Estimación MLE usando trust-ncg (más estable que Newton-CG)\n",
    "res = minimize(log_likelihood, ols_params, args=(X, y), method=\"Newton-CG\", jac=gradient)\n",
    "\n",
    "# Extraer los parámetros estimados\n",
    "params = res.x\n",
    "\n",
    "# Transformar coeficientes a la escala original de las variables\n",
    "print(f\"Longitud de params: {len(params)}\")\n",
    "# Transformar coeficientes a la escala original de las variables\n",
    "params[1:-1] = params[1:-1] / X_std.values[1:]  # Desnormalizar coeficientes excepto el intercepto\n",
    "params[0] -= np.sum(params[1:-1] * X_mean.values[1:] / X_std.values[1:])  # Ajuste del intercepto\n",
    "\n",
    "# Nombres de los parámetros\n",
    "param_names = [\"Intercept\"] + list(df[[\"kl6\", \"k618\", \"wa\", \"we\"]].columns) + [\"sigma\"]\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "results_df = pd.DataFrame({\"Parameter\": param_names, \"Estimate\": params})\n",
    "print(results_df)\n",
    "\n",
    "# Imprimir información de la optimización\n",
    "print(f\"Convergencia: {res.success}\")\n",
    "print(f\"Iteraciones: {res.nit}\")\n",
    "print(f\"Valor de la función de verosimilitud: {res.fun}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# Preparar los datos\n",
    "X = sm.add_constant(df[[\"kl6\", \"k618\", \"wa\", \"we\"]])  # Datos para las variables independientes\n",
    "y = df[\"whrs\"]  # Variable dependiente\n",
    "\n",
    "# Definir función de verosimilitud con censura en 0\n",
    "def log_likelihood(params, X, y):\n",
    "    beta = params[:-1]\n",
    "    sigma = np.abs(params[-1])  # Restricción de sigma > 0\n",
    "    \n",
    "    mu = X @ beta\n",
    "    \n",
    "    # Usar un epsilon pequeño para evitar logaritmos de cero\n",
    "    epsilon = 1e-100\n",
    "    pdf_values = norm.pdf(y, mu, sigma)\n",
    "    cdf_values = norm.cdf(0, mu, sigma)\n",
    "    \n",
    "    # Log-verosimilitud para observaciones no censuradas (y > 0)\n",
    "    log_pdf = np.log(np.maximum(pdf_values, epsilon))\n",
    "    \n",
    "    # Log-verosimilitud para observaciones censuradas en 0 (y == 0)\n",
    "    log_cdf = np.log(np.maximum(cdf_values, epsilon))\n",
    "    \n",
    "    # La log-verosimilitud total\n",
    "    ll = np.sum((y > 0) * log_pdf) + np.sum((y == 0) * log_cdf)\n",
    "    \n",
    "    return -np.sum(ll)  # Se minimiza la log-verosimilitud negativa\n",
    "\n",
    "# Estimación MLE\n",
    "init_params = np.ones(X.shape[1] + 1)  # Inicialización de parámetros\n",
    "res = minimize(log_likelihood, init_params, args=(X, y), method=\"L-BFGS-B\")\n",
    "params = res.x\n",
    "\n",
    "# Verificar la cantidad de parámetros y ajustarlos\n",
    "param_names = [\"Intercept\"] + list(df[[\"kl6\", \"k618\", \"wa\", \"we\"]].columns) + [\"sigma\"]\n",
    "\n",
    "# Comprobar la longitud de ambos\n",
    "print(\"Número de parámetros estimados:\", len(params))\n",
    "print(\"Número de nombres de parámetros:\", len(param_names))\n",
    "\n",
    "# Si el número de parámetros es correcto, crear el DataFrame de resultados\n",
    "if len(params) == len(param_names):\n",
    "    results_df = pd.DataFrame({\"Parameter\": param_names, \"Estimate\": params})\n",
    "    print(results_df)\n",
    "else:\n",
    "    # Si hay un desajuste, ajustamos la cantidad de parámetros\n",
    "    print(f\"Desajuste en la longitud de parámetros: {len(params)} vs {len(param_names)}.\")\n",
    "    if len(params) > len(param_names):\n",
    "        param_names += [\"extra_param_\" + str(i) for i in range(len(param_names), len(params))]\n",
    "    elif len(params) < len(param_names):\n",
    "        param_names = param_names[:len(params)]  # Recortamos si es necesario\n",
    "    \n",
    "    # Crear DataFrame ajustado\n",
    "    results_df = pd.DataFrame({\"Parameter\": param_names, \"Estimate\": params})\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de parámetros estimados: 6\n",
      "Número de nombres de parámetros: 6\n",
      "   Parameter     Estimate\n",
      "0  Intercept   590.431073\n",
      "1        kl6  -827.875466\n",
      "2       k618  -140.078680\n",
      "3         wa   -24.994961\n",
      "4         we   103.635018\n",
      "5      sigma -1309.961567\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# Preparar los datos\n",
    "X = sm.add_constant(df[[\"kl6\", \"k618\", \"wa\", \"we\"]])  # Datos para las variables independientes\n",
    "y = df[\"whrs\"]  # Variable dependiente\n",
    "\n",
    "# Definir función de verosimilitud con truncamiento en 0\n",
    "def log_likelihood_truncation(params, X, y):\n",
    "    beta = params[:-1]\n",
    "    sigma = np.abs(params[-1])  # Restricción de sigma > 0\n",
    "    \n",
    "    mu = X @ beta\n",
    "    \n",
    "    # Usar un epsilon pequeño para evitar logaritmos de cero\n",
    "    epsilon = 1e-100\n",
    "    pdf_values = norm.pdf(y, mu, sigma)\n",
    "    cdf_values = norm.cdf(0, mu, sigma)\n",
    "    \n",
    "    # Log-verosimilitud para observaciones no truncadas (y > 0)\n",
    "    log_pdf = np.log(np.maximum(pdf_values, epsilon))\n",
    "    \n",
    "    # Log-verosimilitud para observaciones truncadas en 0 (y <= 0)\n",
    "    log_cdf = np.log(np.maximum(cdf_values, epsilon))\n",
    "    \n",
    "    # La log-verosimilitud total considerando truncamiento\n",
    "    ll = np.sum((y > 0) * log_pdf) + np.sum((y <= 0) * log_cdf)\n",
    "    \n",
    "    return -np.sum(ll)  # Se minimiza la log-verosimilitud negativa\n",
    "\n",
    "# Estimación MLE con truncamiento\n",
    "init_params = np.ones(X.shape[1] + 1)  # Inicialización de parámetros\n",
    "res = minimize(log_likelihood_truncation, init_params, args=(X, y), method=\"L-BFGS-B\")\n",
    "params = res.x\n",
    "\n",
    "# Verificar la cantidad de parámetros y ajustarlos\n",
    "param_names = [\"Intercept\"] + list(df[[\"kl6\", \"k618\", \"wa\", \"we\"]].columns) + [\"sigma\"]\n",
    "\n",
    "# Comprobar la longitud de ambos\n",
    "print(\"Número de parámetros estimados:\", len(params))\n",
    "print(\"Número de nombres de parámetros:\", len(param_names))\n",
    "\n",
    "# Si el número de parámetros es correcto, crear el DataFrame de resultados\n",
    "if len(params) == len(param_names):\n",
    "    results_df = pd.DataFrame({\"Parameter\": param_names, \"Estimate\": params})\n",
    "    print(results_df)\n",
    "else:\n",
    "    # Si hay un desajuste, ajustamos la cantidad de parámetros\n",
    "    print(f\"Desajuste en la longitud de parámetros: {len(params)} vs {len(param_names)}.\")\n",
    "    if len(params) > len(param_names):\n",
    "        param_names += [\"extra_param_\" + str(i) for i in range(len(param_names), len(params))]\n",
    "    elif len(params) < len(param_names):\n",
    "        param_names = param_names[:len(params)]  # Recortamos si es necesario\n",
    "    \n",
    "    # Crear DataFrame ajustado\n",
    "    results_df = pd.DataFrame({\"Parameter\": param_names, \"Estimate\": params})\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de X: (250, 5)\n",
      "Dimensiones de y: (250,)\n"
     ]
    },
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (slice(None, None, None), 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Estimación MLE utilizando el método trust-ncg (Newton-CG)\u001b[39;00m\n\u001b[0;32m     92\u001b[0m init_params \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Inicialización de parámetros\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_likelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrust-ncg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_likelihood_jacobian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_likelihood_hessian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m params \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mx\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Verificar la cantidad de parámetros y ajustarlos\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:753\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    750\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_dogleg(fun, x0, args, jac, hess,\n\u001b[0;32m    751\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust-ncg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 753\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_trust_ncg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhessp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust-krylov\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    756\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_trust_krylov(fun, x0, args, jac, hess, hessp,\n\u001b[0;32m    757\u001b[0m                                  callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_trustregion_ncg.py:37\u001b[0m, in \u001b[0;36m_minimize_trust_ncg\u001b[1;34m(fun, x0, args, jac, hess, hessp, **trust_region_options)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hessp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEither the Hessian or the Hessian-vector product \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     36\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis required for Newton-CG trust-region minimization\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_minimize_trust_region\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mhessp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhessp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubproblem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCGSteihaugSubproblem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrust_region_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_trustregion.py:175\u001b[0m, in \u001b[0;36m_minimize_trust_region\u001b[1;34m(fun, x0, args, jac, hess, hessp, subproblem, initial_trust_radius, max_trust_radius, eta, gtol, maxiter, disp, return_all, callback, inexact, **unknown_options)\u001b[0m\n\u001b[0;32m    171\u001b[0m x0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x0)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# A ScalarFunction representing the problem. This caches calls to fun, jac,\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# hess.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m fun \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun\n\u001b[0;32m    177\u001b[0m jac \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:288\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    284\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:235\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# Hessian evaluation\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(hess):\n\u001b[1;32m--> 235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_hess, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nhev, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH \u001b[38;5;241m=\u001b[39m \u001b[43m_wrapper_hess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hess \u001b[38;5;129;01min\u001b[39;00m FD_METHODS:\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:56\u001b[0m, in \u001b[0;36m_wrapper_hess\u001b[1;34m(hess, grad, x0, args, finite_diff_options)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper_hess\u001b[39m(hess, grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, x0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, args\u001b[38;5;241m=\u001b[39m(), finite_diff_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(hess):\n\u001b[1;32m---> 56\u001b[0m         H \u001b[38;5;241m=\u001b[39m \u001b[43mhess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m         ncalls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sps\u001b[38;5;241m.\u001b[39missparse(H):\n",
      "Cell \u001b[1;32mIn[19], line 79\u001b[0m, in \u001b[0;36mlog_likelihood_hessian\u001b[1;34m(params, X, y)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m---> 79\u001b[0m         hess[i, j] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum((y \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m (\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m X[:, j]) \u001b[38;5;241m/\u001b[39m sigma\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m pdf_values)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Segunda derivada con respecto a sigma (parte de la Hessiana de sigma)\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3811\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[1;32m-> 3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m   3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: (slice(None, None, None), 0)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# Verificar dimensiones de X y y\n",
    "print(\"Dimensiones de X:\", X.shape)\n",
    "print(\"Dimensiones de y:\", y.shape)\n",
    "\n",
    "# Asegurarse de que X tenga forma (n_samples, n_features)\n",
    "X = sm.add_constant(df[[\"kl6\", \"k618\", \"wa\", \"we\"]])  # Datos para las variables independientes\n",
    "y = df[\"whrs\"]  # Variable dependiente\n",
    "\n",
    "# Asegurarse de que y esté como un vector de una dimensión\n",
    "y = y.values  # Convertir a un arreglo NumPy si es un DataFrame de pandas\n",
    "\n",
    "# Definir función de verosimilitud con censura en 0\n",
    "def log_likelihood(params, X, y):\n",
    "    beta = params[:-1]\n",
    "    sigma = np.abs(params[-1])  # Restricción de sigma > 0\n",
    "    \n",
    "    mu = X @ beta\n",
    "    \n",
    "    # Usar un epsilon pequeño para evitar logaritmos de cero\n",
    "    epsilon = 1e-100\n",
    "    pdf_values = norm.pdf(y, mu, sigma)\n",
    "    cdf_values = norm.cdf(0, mu, sigma)\n",
    "    \n",
    "    # Log-verosimilitud para observaciones no censuradas (y > 0)\n",
    "    log_pdf = np.log(np.maximum(pdf_values, epsilon))\n",
    "    \n",
    "    # Log-verosimilitud para observaciones censuradas en 0 (y == 0)\n",
    "    log_cdf = np.log(np.maximum(cdf_values, epsilon))\n",
    "    \n",
    "    # La log-verosimilitud total\n",
    "    ll = np.sum((y > 0) * log_pdf) + np.sum((y == 0) * log_cdf)\n",
    "    \n",
    "    return -np.sum(ll)  # Se minimiza la log-verosimilitud negativa\n",
    "\n",
    "# Definir el jacobiano de la log-verosimilitud\n",
    "def log_likelihood_jacobian(params, X, y):\n",
    "    beta = params[:-1]\n",
    "    sigma = np.abs(params[-1])  # Restricción de sigma > 0\n",
    "    \n",
    "    mu = X @ beta\n",
    "    \n",
    "    pdf_values = norm.pdf(y, mu, sigma)\n",
    "    cdf_values = norm.cdf(0, mu, sigma)\n",
    "    \n",
    "    # Gradiente con respecto a los parámetros beta\n",
    "    d_ll_dbeta = np.sum((y > 0) * (y - mu) / sigma**2 * X, axis=0)\n",
    "    \n",
    "    # Gradiente con respecto a sigma\n",
    "    d_ll_dsigma = np.sum((y > 0) * (mu - y)**2 * pdf_values / (sigma**3) - pdf_values / sigma)\n",
    "    \n",
    "    # Concatenar el gradiente\n",
    "    grad = np.concatenate([d_ll_dbeta, [d_ll_dsigma]])\n",
    "    \n",
    "    return -grad  # Se minimiza, por lo que el gradiente debe ser negativo\n",
    "\n",
    "# Definir el hessiano de la log-verosimilitud\n",
    "def log_likelihood_hessian(params, X, y):\n",
    "    beta = params[:-1]\n",
    "    sigma = np.abs(params[-1])  # Restricción de sigma > 0\n",
    "    \n",
    "    mu = X @ beta\n",
    "    \n",
    "    pdf_values = norm.pdf(y, mu, sigma)\n",
    "    cdf_values = norm.cdf(0, mu, sigma)\n",
    "    \n",
    "    # Inicializamos la matriz Hessiana de tamaño (n_params, n_params), donde n_params = len(beta) + 1\n",
    "    n_params = X.shape[1] + 1  # Número de parámetros (coeficientes beta + sigma)\n",
    "    hess = np.zeros((n_params, n_params))\n",
    "    \n",
    "    # Segunda derivada con respecto a los parámetros beta (parte de la Hessiana de beta)\n",
    "    for i in range(X.shape[1]):\n",
    "        for j in range(X.shape[1]):\n",
    "            hess[i, j] = np.sum((y > 0) * (X[:, i] * X[:, j]) / sigma**2 * pdf_values)\n",
    "\n",
    "    # Segunda derivada con respecto a sigma (parte de la Hessiana de sigma)\n",
    "    for i in range(X.shape[1]):\n",
    "        hess[i, -1] = np.sum((y > 0) * (X[:, i] * (mu - y)) / sigma**3 * pdf_values)\n",
    "        hess[-1, i] = hess[i, -1]  # La Hessiana es simétrica\n",
    "    \n",
    "    # Segunda derivada con respecto a sigma (d2_ll_dsigma2)\n",
    "    hess[-1, -1] = np.sum((y > 0) * (mu - y)**2 * pdf_values / (sigma**3)) - np.sum(pdf_values / sigma)\n",
    "    \n",
    "    return hess\n",
    "\n",
    "# Estimación MLE utilizando el método trust-ncg (Newton-CG)\n",
    "init_params = np.ones(X.shape[1] + 1)  # Inicialización de parámetros\n",
    "res = minimize(log_likelihood, init_params, args=(X, y), method=\"trust-ncg\", jac=log_likelihood_jacobian, hess=log_likelihood_hessian)\n",
    "params = res.x\n",
    "\n",
    "# Verificar la cantidad de parámetros y ajustarlos\n",
    "param_names = [\"Intercept\"] + list(df[[\"kl6\", \"k618\", \"wa\", \"we\"]].columns) + [\"sigma\"]\n",
    "\n",
    "# Comprobar la longitud de ambos\n",
    "print(\"Número de parámetros estimados:\", len(params))\n",
    "print(\"Número de nombres de parámetros:\", len(param_names))\n",
    "\n",
    "# Si el número de parámetros es correcto, crear el DataFrame de resultados\n",
    "if len(params) == len(param_names):\n",
    "    results_df = pd.DataFrame({\"Parameter\": param_names, \"Estimate\": params})\n",
    "    print(results_df)\n",
    "else:\n",
    "    # Si hay un desajuste, ajustamos la cantidad de parámetros\n",
    "    print(f\"Desajuste en la longitud de parámetros: {len(params)} vs {len(param_names)}.\")\n",
    "    if len(params) > len(param_names):\n",
    "        param_names += [\"extra_param_\" + str(i) for i in range(len(param_names), len(params))]  \n",
    "    elif len(params) < len(param_names):\n",
    "        param_names = param_names[:len(params)]  # Recortamos si es necesario\n",
    "    \n",
    "    # Crear DataFrame ajustado\n",
    "    results_df = pd.DataFrame({\"Parameter\": param_names, \"Estimate\": params})\n",
    "    print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   lfp     250 non-null    int64\n",
      " 1   whrs    250 non-null    int64\n",
      " 2   kl6     250 non-null    int64\n",
      " 3   k618    250 non-null    int64\n",
      " 4   wa      250 non-null    int64\n",
      " 5   we      250 non-null    int64\n",
      "dtypes: int64(6)\n",
      "memory usage: 11.8 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 if woman worked in 1975',\n",
       " \"Wife's hours of work\",\n",
       " '# of children younger than 6',\n",
       " '# of children between 6 and 18',\n",
       " \"Wife's age\",\n",
       " \"Wife's educational attainment\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.column_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lfp\n",
       "1    150\n",
       "0    100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lfp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>whrs</th>\n",
       "      <td>250.0</td>\n",
       "      <td>799.84</td>\n",
       "      <td>915.60348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>406.5</td>\n",
       "      <td>1599.75</td>\n",
       "      <td>4950.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count    mean        std  min  25%    50%      75%     max\n",
       "whrs  250.0  799.84  915.60348  0.0  0.0  406.5  1599.75  4950.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['whrs']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   whrs   R-squared:                       0.072\n",
      "Model:                            OLS   Adj. R-squared:                  0.046\n",
      "Method:                 Least Squares   F-statistic:                     2.802\n",
      "Date:                Sun, 23 Feb 2025   Prob (F-statistic):             0.0281\n",
      "Time:                        00:54:19   Log-Likelihood:                -1214.6\n",
      "No. Observations:                 150   AIC:                             2439.\n",
      "Df Residuals:                     145   BIC:                             2454.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       1629.8168    615.130      2.650      0.009     414.037    2845.597\n",
      "kl6         -421.4822    167.973     -2.509      0.013    -753.475     -89.490\n",
      "k618        -104.4571     54.186     -1.928      0.056    -211.554       2.640\n",
      "wa            -4.7849      9.691     -0.494      0.622     -23.938      14.368\n",
      "we             9.3532     31.238      0.299      0.765     -52.387      71.094\n",
      "==============================================================================\n",
      "Omnibus:                       28.314   Durbin-Watson:                   2.066\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               53.578\n",
      "Skew:                           0.864   Prob(JB):                     2.32e-12\n",
      "Kurtosis:                       5.364   Cond. No.                         425.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#¿Qué sucede si queremos estimar una función de oferta de trabajo?\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Filtrar datos donde whrs > 0\n",
    "df_filtered = df[df[\"whrs\"] > 0]\n",
    "\n",
    "# Definir variables dependiente (Y) e independientes (X)\n",
    "y = df_filtered[\"whrs\"]\n",
    "X = df_filtered[[\"kl6\", \"k618\", \"wa\", \"we\"]]\n",
    "\n",
    "# Agregar constante para la intersección (equivalente a _cons en Stata)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Ajustar el modelo\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Mostrar resultados\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tobit' from 'statsmodels.discrete.count_model' (c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\discrete\\count_model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscrete\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcount_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tobit\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Definir variables dependiente (Y) e independientes (X)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhrs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Tobit' from 'statsmodels.discrete.count_model' (c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\discrete\\count_model.py)"
     ]
    }
   ],
   "source": [
    "from statsmodels.discrete.count_model import Tobit\n",
    "\n",
    "# Definir variables dependiente (Y) e independientes (X)\n",
    "y = df[\"whrs\"]\n",
    "X = df[[\"kl6\", \"k618\", \"wa\", \"we\"]]\n",
    "\n",
    "# Agregar constante para la intersección (equivalente a _cons en Stata)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Ajustar el modelo Tobit con censura inferior en 0\n",
    "model = Tobit(y, X).fit(censor_lower=0)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_numdiff.py:592: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x1) - f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes estimados: [-5200401.23618509  1717029.03177521   313985.12206049    94771.03642374\n",
      "    44524.92114298]\n",
      "Sigma estimado: 510055.98735519126\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Definir variables dependiente (Y) e independientes (X)\n",
    "y = df[\"whrs\"]\n",
    "X = df[[\"kl6\", \"k618\", \"wa\", \"we\"]]\n",
    "\n",
    "# Agregar constante\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Función de log-verosimilitud del modelo Tobit (censura inferior en 0)\n",
    "def tobit_loglik(params, y, X):\n",
    "    beta = params[:-1]  # Coeficientes de regresión\n",
    "    sigma = params[-1]  # Desviación estándar del error\n",
    "    \n",
    "    if sigma <= 0:\n",
    "        return np.inf  # Evitar valores no válidos\n",
    "    \n",
    "    xb = X @ beta  # Producto matricial X * beta\n",
    "    uncensored = (y > 0)\n",
    "    \n",
    "    # Parte no censurada: Log-verosimilitud de la normal\n",
    "    ll_uncensored = -0.5 * np.log(2 * np.pi) - np.log(sigma) - (y - xb) ** 2 / (2 * sigma**2)\n",
    "    \n",
    "    # Parte censurada: Probabilidad acumulada de la normal (usando scipy.stats.norm)\n",
    "    ll_censored = np.log(np.maximum(norm.cdf(xb / sigma), 1e-10))  # Evita log(0)\n",
    "    \n",
    "    # Combinar log-verosimilitudes\n",
    "    loglik = np.where(uncensored, ll_uncensored, ll_censored)\n",
    "    \n",
    "    return -np.sum(loglik)  # Maximizar la verosimilitud minimizando la función de pérdida\n",
    "\n",
    "# Estimación de parámetros iniciales\n",
    "init_params = np.append(np.zeros(X.shape[1]), 1)  # Coeficientes iniciales + sigma\n",
    "\n",
    "# Optimización numérica\n",
    "res = minimize(tobit_loglik, init_params, args=(y, X), method=\"BFGS\")\n",
    "\n",
    "# Resultados\n",
    "beta_est = res.x[:-1]  # Coeficientes estimados\n",
    "sigma_est = res.x[-1]  # Estimación de sigma\n",
    "\n",
    "print(\"Coeficientes estimados:\", beta_est)\n",
    "print(\"Sigma estimado:\", sigma_est)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import scipy.stats\n",
    "from scipy.special import log_ndtr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "def split_left_right_censored(x, y, cens):\n",
    "    counts = cens.value_counts()\n",
    "    if -1 not in counts and 1 not in counts:\n",
    "        warnings.warn(\"No censored observations; use regression methods for uncensored data\")\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for value in [-1, 0, 1]:\n",
    "        if value in counts:\n",
    "            split = cens == value\n",
    "            y_split = np.squeeze(y[split].values)\n",
    "            x_split = x[split].values\n",
    "\n",
    "        else:\n",
    "            y_split, x_split = None, None\n",
    "        xs.append(x_split)\n",
    "        ys.append(y_split)\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "def tobit_neg_log_likelihood(xs, ys, params):\n",
    "    x_left, x_mid, x_right = xs\n",
    "    y_left, y_mid, y_right = ys\n",
    "\n",
    "    b = params[:-1]\n",
    "    # s = math.exp(params[-1])\n",
    "    s = params[-1]\n",
    "\n",
    "    to_cat = []\n",
    "\n",
    "    cens = False\n",
    "    if y_left is not None:\n",
    "        cens = True\n",
    "        left = (y_left - np.dot(x_left, b))\n",
    "        to_cat.append(left)\n",
    "    if y_right is not None:\n",
    "        cens = True\n",
    "        right = (np.dot(x_right, b) - y_right)\n",
    "        to_cat.append(right)\n",
    "    if cens:\n",
    "        concat_stats = np.concatenate(to_cat, axis=0) / s\n",
    "        log_cum_norm = scipy.stats.norm.logcdf(concat_stats)  # log_ndtr(concat_stats)\n",
    "        cens_sum = log_cum_norm.sum()\n",
    "    else:\n",
    "        cens_sum = 0\n",
    "\n",
    "    if y_mid is not None:\n",
    "        mid_stats = (y_mid - np.dot(x_mid, b)) / s\n",
    "        mid = scipy.stats.norm.logpdf(mid_stats) - math.log(max(np.finfo('float').resolution, s))\n",
    "        mid_sum = mid.sum()\n",
    "    else:\n",
    "        mid_sum = 0\n",
    "\n",
    "    loglik = cens_sum + mid_sum\n",
    "\n",
    "    return - loglik\n",
    "\n",
    "\n",
    "def tobit_neg_log_likelihood_der(xs, ys, params):\n",
    "    x_left, x_mid, x_right = xs\n",
    "    y_left, y_mid, y_right = ys\n",
    "\n",
    "    b = params[:-1]\n",
    "    # s = math.exp(params[-1]) # in censReg, not using chain rule as below; they optimize in terms of log(s)\n",
    "    s = params[-1]\n",
    "\n",
    "    beta_jac = np.zeros(len(b))\n",
    "    sigma_jac = 0\n",
    "\n",
    "    if y_left is not None:\n",
    "        left_stats = (y_left - np.dot(x_left, b)) / s\n",
    "        l_pdf = scipy.stats.norm.logpdf(left_stats)\n",
    "        l_cdf = log_ndtr(left_stats)\n",
    "        left_frac = np.exp(l_pdf - l_cdf)\n",
    "        beta_left = np.dot(left_frac, x_left / s)\n",
    "        beta_jac -= beta_left\n",
    "\n",
    "        left_sigma = np.dot(left_frac, left_stats)\n",
    "        sigma_jac -= left_sigma\n",
    "\n",
    "    if y_right is not None:\n",
    "        right_stats = (np.dot(x_right, b) - y_right) / s\n",
    "        r_pdf = scipy.stats.norm.logpdf(right_stats)\n",
    "        r_cdf = log_ndtr(right_stats)\n",
    "        right_frac = np.exp(r_pdf - r_cdf)\n",
    "        beta_right = np.dot(right_frac, x_right / s)\n",
    "        beta_jac += beta_right\n",
    "\n",
    "        right_sigma = np.dot(right_frac, right_stats)\n",
    "        sigma_jac -= right_sigma\n",
    "\n",
    "    if y_mid is not None:\n",
    "        mid_stats = (y_mid - np.dot(x_mid, b)) / s\n",
    "        beta_mid = np.dot(mid_stats, x_mid / s)\n",
    "        beta_jac += beta_mid\n",
    "\n",
    "        mid_sigma = (np.square(mid_stats) - 1).sum()\n",
    "        sigma_jac += mid_sigma\n",
    "\n",
    "    combo_jac = np.append(beta_jac, sigma_jac / s)  # by chain rule, since the expression above is dloglik/dlogsigma\n",
    "\n",
    "    return -combo_jac\n",
    "\n",
    "\n",
    "class TobitModel:\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.ols_coef_ = None\n",
    "        self.ols_intercept = None\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.sigma_ = None\n",
    "\n",
    "    def fit(self, x, y, cens, verbose=False):\n",
    "        \"\"\"\n",
    "        Fit a maximum-likelihood Tobit regression\n",
    "        :param x: Pandas DataFrame (n_samples, n_features): Data\n",
    "        :param y: Pandas Series (n_samples,): Target\n",
    "        :param cens: Pandas Series (n_samples,): -1 indicates left-censored samples, 0 for uncensored, 1 for right-censored\n",
    "        :param verbose: boolean, show info from minimization\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x_copy = x.copy()\n",
    "        if self.fit_intercept:\n",
    "            x_copy.insert(0, 'intercept', 1.0)\n",
    "        else:\n",
    "            x_copy.scale(with_mean=True, with_std=False, copy=False)\n",
    "        init_reg = LinearRegression(fit_intercept=False).fit(x_copy, y)\n",
    "        b0 = init_reg.coef_\n",
    "        y_pred = init_reg.predict(x_copy)\n",
    "        resid = y - y_pred\n",
    "        resid_var = np.var(resid)\n",
    "        s0 = np.sqrt(resid_var)\n",
    "        params0 = np.append(b0, s0)\n",
    "        xs, ys = split_left_right_censored(x_copy, y, cens)\n",
    "\n",
    "        result = minimize(lambda params: tobit_neg_log_likelihood(xs, ys, params), params0, method='BFGS',\n",
    "                          jac=lambda params: tobit_neg_log_likelihood_der(xs, ys, params), options={'disp': verbose})\n",
    "        if verbose:\n",
    "            print(result)\n",
    "        self.ols_coef_ = b0[1:]\n",
    "        self.ols_intercept = b0[0]\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = result.x[1]\n",
    "            self.coef_ = result.x[1:-1]\n",
    "        else:\n",
    "            self.coef_ = result.x[:-1]\n",
    "            self.intercept_ = 0\n",
    "        self.sigma_ = result.x[-1]\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.intercept_ + np.dot(x, self.coef_)\n",
    "\n",
    "    def score(self, x, y, scoring_function=mean_absolute_error):\n",
    "        y_pred = np.dot(x, self.coef_)\n",
    "        return scoring_function(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 294.44827939, -827.78090417, -140.01430074,  -24.97854148,\n",
       "        103.69518044])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[\"whrs\"]\n",
    "X = df[[\"kl6\", \"k618\", \"wa\", \"we\"]]\n",
    "X = sm.add_constant(X)\n",
    "df['cens'] = np.where(df['whrs'] == 0, -1, 0)\n",
    "cens = df['cens']\n",
    "tr = TobitModel()\n",
    "tr = tr.fit(X, y, cens, verbose=False)\n",
    "tr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7068\\1321454343.py:16: UserWarning: No censored observations; use regression methods for uncensored data\n",
      "  warnings.warn(\"No censored observations; use regression methods for uncensored data\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 470.02964199, -462.12329817,  -91.14099631,  -13.15770391,\n",
       "         53.26155967])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"whrs\"] = df[\"whrs\"].apply(lambda x: max(x, 0))\n",
    "y = df[\"whrs\"]\n",
    "X = df[[\"kl6\", \"k618\", \"wa\", \"we\"]]\n",
    "X = sm.add_constant(X)\n",
    "df['cens'] = np.where(df['whrs'] == 0, 0, 0)\n",
    "cens = df['cens']\n",
    "tr = TobitModel()\n",
    "tr = tr.fit(X, y, cens, verbose=False)\n",
    "tr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_21004\\3479087987.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[y < 0] = 0  # Censuramos en 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (250,5) and (4,) not aligned: 5 (dim 1) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m X \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39madd_constant(X)\n\u001b[0;32m     33\u001b[0m model \u001b[38;5;241m=\u001b[39m Tobit(y, X, left\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39msummary())\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:1015\u001b[0m, in \u001b[0;36mGenericLikelihoodModel.fit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, callback, retall, **kwargs)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcov_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnonrobust\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1014\u001b[0m fit_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit\n\u001b[1;32m-> 1015\u001b[0m mlefit \u001b[38;5;241m=\u001b[39m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m results_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults_class\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1021\u001b[0m                         GenericLikelihoodModelResults)\n\u001b[0;32m   1022\u001b[0m genericmlefit \u001b[38;5;241m=\u001b[39m results_class(\u001b[38;5;28mself\u001b[39m, mlefit)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:566\u001b[0m, in \u001b[0;36mLikelihoodModel.fit\u001b[1;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_t\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    565\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Optimizer()\n\u001b[1;32m--> 566\u001b[0m xopt, retvals, optim_settings \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mhessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mretall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;66;03m# Restore cov_type, cov_kwds and use_t\u001b[39;00m\n\u001b[0;32m    576\u001b[0m optim_settings\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\optimizer.py:243\u001b[0m, in \u001b[0;36mOptimizer._fit\u001b[1;34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[0m\n\u001b[0;32m    240\u001b[0m     fit_funcs\u001b[38;5;241m.\u001b[39mupdate(extra_fit_funcs)\n\u001b[0;32m    242\u001b[0m func \u001b[38;5;241m=\u001b[39m fit_funcs[method]\n\u001b[1;32m--> 243\u001b[0m xopt, retvals \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mretall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mhess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhessian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m optim_settings \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: method, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_params\u001b[39m\u001b[38;5;124m'\u001b[39m: start_params,\n\u001b[0;32m    249\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: maxiter, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_output\u001b[39m\u001b[38;5;124m'\u001b[39m: full_output,\n\u001b[0;32m    250\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfargs\u001b[39m\u001b[38;5;124m'\u001b[39m: fargs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m: callback,\n\u001b[0;32m    251\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretall\u001b[39m\u001b[38;5;124m'\u001b[39m: retall, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_fit_funcs\u001b[39m\u001b[38;5;124m\"\u001b[39m: extra_fit_funcs}\n\u001b[0;32m    252\u001b[0m optim_settings\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\optimizer.py:737\u001b[0m, in \u001b[0;36m_fit_nm\u001b[1;34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess)\u001b[0m\n\u001b[0;32m    735\u001b[0m ftol \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mftol\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m    736\u001b[0m maxfun \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxfun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 737\u001b[0m retvals \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mftol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mftol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxfun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxfun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_output:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m retall:\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:653\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(func, x0, args, xtol, ftol, maxiter, maxfun, full_output, disp, retall, callback, initial_simplex)\u001b[0m\n\u001b[0;32m    644\u001b[0m opts \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxatol\u001b[39m\u001b[38;5;124m'\u001b[39m: xtol,\n\u001b[0;32m    645\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfatol\u001b[39m\u001b[38;5;124m'\u001b[39m: ftol,\n\u001b[0;32m    646\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: maxiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn_all\u001b[39m\u001b[38;5;124m'\u001b[39m: retall,\n\u001b[0;32m    650\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial_simplex\u001b[39m\u001b[38;5;124m'\u001b[39m: initial_simplex}\n\u001b[0;32m    652\u001b[0m callback \u001b[38;5;241m=\u001b[39m _wrap_callback(callback)\n\u001b[1;32m--> 653\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_neldermead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_output:\n\u001b[0;32m    655\u001b[0m     retlist \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfev\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:817\u001b[0m, in \u001b[0;36m_minimize_neldermead\u001b[1;34m(func, x0, args, callback, maxiter, maxfev, disp, return_all, initial_simplex, xatol, fatol, adaptive, bounds, **unknown_options)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 817\u001b[0m         fsim[k] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _MaxFuncCallError:\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:526\u001b[0m, in \u001b[0;36m_wrap_scalar_function_maxfun_validation.<locals>.function_wrapper\u001b[1;34m(x, *wrapper_args)\u001b[0m\n\u001b[0;32m    524\u001b[0m ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;66;03m# A copy of x is sent to the user function (gh13740)\u001b[39;00m\n\u001b[1;32m--> 526\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwrapper_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;66;03m# Ideally, we'd like to a have a true scalar returned from f(x). For\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;66;03m# backwards-compatibility, also allow np.array([1.3]),\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;66;03m# np.array([[1.3]]) etc.\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:534\u001b[0m, in \u001b[0;36mLikelihoodModel.fit.<locals>.f\u001b[1;34m(params, *args)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(params, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m--> 534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m nobs\n",
      "Cell \u001b[1;32mIn[23], line 16\u001b[0m, in \u001b[0;36mTobit.loglike\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m     13\u001b[0m sigma \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# La desviación estándar (último parámetro)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Predicciones lineales\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m xb \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendog\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Calcular la log-verosimilitud\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (250,5) and (4,) not aligned: 5 (dim 1) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "\n",
    "class Tobit(GenericLikelihoodModel):\n",
    "    def __init__(self, endog, exog, left=0, **kwds):\n",
    "        self.left = left\n",
    "        super().__init__(endog, exog, **kwds)\n",
    "\n",
    "    def loglike(self, params):\n",
    "        # Separar beta (coeficientes) y sigma (desviación estándar)\n",
    "        beta = params[:-1]  # Los coeficientes\n",
    "        sigma = params[-1]  # La desviación estándar (último parámetro)\n",
    "        \n",
    "        # Predicciones lineales\n",
    "        xb = np.dot(self.exog, beta)\n",
    "        y = self.endog\n",
    "\n",
    "        # Calcular la log-verosimilitud\n",
    "        ll = np.where(y > self.left,\n",
    "                      np.log(1 / sigma) + sm.distributions.norm.logpdf((y - xb) / sigma),\n",
    "                      sm.distributions.norm.logcdf((self.left - xb) / sigma))\n",
    "        \n",
    "        return ll.sum()\n",
    "\n",
    "# Cargar datos\n",
    "y = df[\"whrs\"]\n",
    "X = df[[\"kl6\", \"k618\", \"wa\", \"we\"]]\n",
    "y[y < 0] = 0  # Censuramos en 0\n",
    "\n",
    "# Ajustar modelo\n",
    "X = sm.add_constant(X)\n",
    "model = Tobit(y, X, left=0)\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 5.468361\n",
      "         Iterations: 566\n",
      "         Function evaluations: 894\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Tobit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>                 <td>whrs</td>        <th>  Pseudo R-squ:      </th>   <td>0.008</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>Maximum Likelihood</td> <th>  Log-Likelihood:    </th>  <td>-1367.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>             <td>   250</td>       <th>  LL-Null:           </th>  <td>-1378.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Uncensored Obs:</th>             <td>150</td>        <th>  LL-Ratio:          </th>   <td>23.0</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Left-censored Obs:</th>          <td>100</td>        <th>  LLR p-value:       </th>   <td>0.000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Right-censored Obs:</th>          <td>0</td>         <th>  AIC:               </th>  <td>2744.2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>                 <td>   245</td>       <th>  BIC:               </th>  <td>2761.8</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>                     <td>     4</td>       <th>  Covariance Type:   </th> <td>nonrobust</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td>  589.0003</td> <td>  841.589</td> <td>    0.700</td> <td> 0.484</td> <td>-1060.483</td> <td> 2238.484</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kl6</th>        <td> -827.7658</td> <td>  214.752</td> <td>   -3.855</td> <td> 0.000</td> <td>-1248.672</td> <td> -406.860</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>k618</th>       <td> -140.0192</td> <td>   74.227</td> <td>   -1.886</td> <td> 0.059</td> <td> -285.502</td> <td>    5.463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wa</th>         <td>  -24.9792</td> <td>   13.257</td> <td>   -1.884</td> <td> 0.060</td> <td>  -50.963</td> <td>    1.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>we</th>         <td>  103.6896</td> <td>   41.826</td> <td>    2.479</td> <td> 0.013</td> <td>   21.712</td> <td>  185.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Log(Sigma)</th> <td>    7.1777</td> <td>    0.063</td> <td>  113.628</td> <td> 0.000</td> <td>    7.054</td> <td>    7.302</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}          &        whrs        & \\textbf{  Pseudo R-squ:      } &   0.008     \\\\\n",
       "\\textbf{Method:}                 & Maximum Likelihood & \\textbf{  Log-Likelihood:    } &  -1367.1    \\\\\n",
       "\\textbf{No. Observations:}       &          250       & \\textbf{  LL-Null:           } &  -1378.6    \\\\\n",
       "\\textbf{No. Uncensored Obs:}     &        150         & \\textbf{  LL-Ratio:          } &    23.0     \\\\\n",
       "\\textbf{No. Left-censored Obs:}  &        100         & \\textbf{  LLR p-value:       } &   0.000     \\\\\n",
       "\\textbf{No. Right-censored Obs:} &         0          & \\textbf{  AIC:               } &   2744.2    \\\\\n",
       "\\textbf{Df Residuals:}           &          245       & \\textbf{  BIC:               } &   2761.8    \\\\\n",
       "\\textbf{Df Model:}               &            4       & \\textbf{  Covariance Type:   } & nonrobust   \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                    & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}      &     589.0003  &      841.589     &     0.700  &         0.484        &    -1060.483    &     2238.484     \\\\\n",
       "\\textbf{kl6}        &    -827.7658  &      214.752     &    -3.855  &         0.000        &    -1248.672    &     -406.860     \\\\\n",
       "\\textbf{k618}       &    -140.0192  &       74.227     &    -1.886  &         0.059        &     -285.502    &        5.463     \\\\\n",
       "\\textbf{wa}         &     -24.9792  &       13.257     &    -1.884  &         0.060        &      -50.963    &        1.004     \\\\\n",
       "\\textbf{we}         &     103.6896  &       41.826     &     2.479  &         0.013        &       21.712    &      185.668     \\\\\n",
       "\\textbf{Log(Sigma)} &       7.1777  &        0.063     &   113.628  &         0.000        &        7.054    &        7.302     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Tobit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                              Tobit Regression Results                             \n",
       "===================================================================================\n",
       "Dep. Variable:                        whrs   Pseudo R-squ:                    0.008\n",
       "Method:                 Maximum Likelihood   Log-Likelihood:                -1367.1\n",
       "No. Observations:                      250   LL-Null:                       -1378.6\n",
       "No. Uncensored Obs:                    150   LL-Ratio:                         23.0\n",
       "No. Left-censored Obs:                 100   LLR p-value:                     0.000\n",
       "No. Right-censored Obs:                  0   AIC:                            2744.2\n",
       "Df Residuals:                          245   BIC:                            2761.8\n",
       "Df Model:                                4   Covariance Type:             nonrobust\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        589.0003    841.589      0.700      0.484   -1060.483    2238.484\n",
       "kl6         -827.7658    214.752     -3.855      0.000   -1248.672    -406.860\n",
       "k618        -140.0192     74.227     -1.886      0.059    -285.502       5.463\n",
       "wa           -24.9792     13.257     -1.884      0.060     -50.963       1.004\n",
       "we           103.6896     41.826      2.479      0.013      21.712     185.668\n",
       "Log(Sigma)     7.1777      0.063    113.628      0.000       7.054       7.302\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from py4etrics.tobit import Tobit\n",
    "import statsmodels.api as sm\n",
    "\n",
    "y = df[\"whrs\"]\n",
    "x = df[[\"kl6\", \"k618\", \"wa\", \"we\"]]\n",
    "X = sm.add_constant(x)\n",
    "df['cens'] = np.where(df['whrs'] == 0, -1, 0)\n",
    "cens = df['cens']\n",
    "model = Tobit(y, X, cens, left=0).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 7.658411\n",
      "         Iterations: 4994\n",
      "         Function evaluations: 7886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\base\\model.py:595: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Truncreg Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>whrs</td>        <th>  Pseudo R-squ:      </th>   <td>0.003</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Truncreg</td>      <th>  Log-Likelihood:    </th>  <td>-1914.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>           <td>Maximum Likelihood</td> <th>  LL-Null:           </th>  <td>-1921.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>              <td>Sun, 23 Feb 2025</td>  <th>  LL-Ratio:          </th>   <td>13.0</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                  <td>00:16:22</td>      <th>  LLR p-value:       </th>   <td>0.011</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>       <td>   250</td>       <th>  AIC:               </th>  <td>3839.2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>           <td>   245</td>       <th>  BIC:               </th>  <td>3856.8</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>               <td>     4</td>       <th>  Covariance Type:   </th> <td>nonrobust</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>      <td>-1.664e+10</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>kl6</th>        <td>-5.197e+09</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>k618</th>       <td>-1.261e+08</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wa</th>         <td>-8.649e+07</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>we</th>         <td> 1.056e+08</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Log(Sigma)</th> <td>   15.1967</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        whrs        & \\textbf{  Pseudo R-squ:      } &   0.003     \\\\\n",
       "\\textbf{Model:}            &      Truncreg      & \\textbf{  Log-Likelihood:    } &  -1914.6    \\\\\n",
       "\\textbf{Method:}           & Maximum Likelihood & \\textbf{  LL-Null:           } &  -1921.1    \\\\\n",
       "\\textbf{Date:}             &  Sun, 23 Feb 2025  & \\textbf{  LL-Ratio:          } &    13.0     \\\\\n",
       "\\textbf{Time:}             &      00:16:22      & \\textbf{  LLR p-value:       } &   0.011     \\\\\n",
       "\\textbf{No. Observations:} &          250       & \\textbf{  AIC:               } &   3839.2    \\\\\n",
       "\\textbf{Df Residuals:}     &          245       & \\textbf{  BIC:               } &   3856.8    \\\\\n",
       "\\textbf{Df Model:}         &            4       & \\textbf{  Covariance Type:   } & nonrobust   \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                    & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}      &   -1.664e+10  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n",
       "\\textbf{kl6}        &   -5.197e+09  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n",
       "\\textbf{k618}       &   -1.261e+08  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n",
       "\\textbf{wa}         &   -8.649e+07  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n",
       "\\textbf{we}         &    1.056e+08  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n",
       "\\textbf{Log(Sigma)} &      15.1967  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Truncreg Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                         Truncreg Regression Results                          \n",
       "==============================================================================\n",
       "Dep. Variable:                   whrs   Pseudo R-squ:                    0.003\n",
       "Model:                       Truncreg   Log-Likelihood:                -1914.6\n",
       "Method:            Maximum Likelihood   LL-Null:                       -1921.1\n",
       "Date:                Sun, 23 Feb 2025   LL-Ratio:                         13.0\n",
       "Time:                        00:16:22   LLR p-value:                     0.011\n",
       "No. Observations:                 250   AIC:                            3839.2\n",
       "Df Residuals:                     245   BIC:                            3856.8\n",
       "Df Model:                           4   Covariance Type:             nonrobust\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const      -1.664e+10        nan        nan        nan         nan         nan\n",
       "kl6        -5.197e+09        nan        nan        nan         nan         nan\n",
       "k618       -1.261e+08        nan        nan        nan         nan         nan\n",
       "wa         -8.649e+07        nan        nan        nan         nan         nan\n",
       "we          1.056e+08        nan        nan        nan         nan         nan\n",
       "Log(Sigma)    15.1967        nan        nan        nan         nan         nan\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from py4etrics.truncreg import Truncreg\n",
    "import statsmodels.api as sm\n",
    "\n",
    "y = df[\"whrs\"]\n",
    "x = df[[\"kl6\", \"k618\", \"wa\", \"we\"]]\n",
    "X = sm.add_constant(x)\n",
    "model = Truncreg(y, X, left=0).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import Bounds, minimize\n",
    "import re\n",
    "\n",
    "def truncreg(formula, data, point, direction, scaled=False, iterlim=50):\n",
    "    if direction not in [\"left\", \"right\"]:\n",
    "        raise ValueError(\"'direction' must be 'left' or 'right'\")\n",
    "    \n",
    "    x_vars = re.split(r'[*+]', re.search(r'~\\s*(.+)', formula).group(1).strip())\n",
    "    x_vars = [var.strip() for var in x_vars]\n",
    "    x = data[[var for var in x_vars if var]]\n",
    "    y = data[formula.split('~')[0].strip()]\n",
    "    \n",
    "    intercept = np.ones((x.shape[0], 1))\n",
    "    x = np.hstack((intercept, x))\n",
    "    \n",
    "    if direction == \"left\" and any(y < point):\n",
    "        raise ValueError(\"Data non-truncated, contains observations < '{}'\".format(point))\n",
    "    if direction == \"right\" and any(y > point):\n",
    "        raise ValueError(\"Data non-truncated, contains observations > '{}'\".format(point))\n",
    "\n",
    "    def maxLikTruncreg(param, x, y, point, direction, scaled=scaled):\n",
    "        epsilon = 1e-8\n",
    "        sign = 1 if direction == \"left\" else -1\n",
    "        beta = param[:-1]\n",
    "        sigma = param[-1]\n",
    "        bX = np.dot(x, beta)\n",
    "        \n",
    "        if not scaled:\n",
    "            resid = y - bX\n",
    "            trunc = bX - point\n",
    "    \n",
    "            #Potential Overflow Handling\n",
    "            exp_argument = norm.logpdf(sign * trunc / (sigma + epsilon)) - norm.logcdf(sign * trunc / (sigma + epsilon))\n",
    "            exp_argument = np.clip(exp_argument, a_min=None, a_max=700)\n",
    "            mills = np.exp(exp_argument)\n",
    "    \n",
    "            lnL = -np.log(sigma + epsilon) + norm.logpdf(resid / (sigma + epsilon)) - norm.logcdf(sign * trunc / (sigma + epsilon))\n",
    "            logLik = lnL.sum() if isinstance(lnL, np.ndarray) and lnL.ndim > 0 else lnL\n",
    "    \n",
    "            # Gradient calculation\n",
    "            gbX = resid / (sigma ** 2 + epsilon) - sign * mills / (sigma + epsilon)\n",
    "            gsigma = -1 / (sigma + epsilon) + resid ** 2 / (sigma + epsilon) ** 3 + sign * mills * trunc / (sigma + epsilon) ** 2\n",
    "            gradient = np.concatenate([gbX, gsigma])\n",
    "    \n",
    "            # Hessian calculation\n",
    "            bb = -1 / (sigma ** 2 + epsilon) + mills * (sign * trunc / (sigma + epsilon) + mills) / (sigma + epsilon) ** 2\n",
    "            ss = 1 / (sigma + epsilon) ** 2 - 3 * resid ** 2 / (sigma + epsilon) ** 4 - 2 * mills * sign * trunc / (sigma + epsilon) ** 3 + \\\n",
    "                 mills * (sign * trunc / (sigma + epsilon) + mills) * trunc ** 2 / (sigma + epsilon) ** 4\n",
    "            bs = -2 * resid / (sigma + epsilon) ** 3 + sign * mills / (sigma + epsilon) ** 2 - \\\n",
    "                 mills * (mills + sign * trunc / (sigma + epsilon)) * trunc / (sigma + epsilon) ** 3\n",
    "            bs = np.array(bs)\n",
    "    \n",
    "            bb_matrix = np.dot(x.T, np.reshape(bb, (-1, 1)) * x)\n",
    "            bs_vector = np.sum(np.reshape(bs, (-1, 1)) * x, axis=0)\n",
    "            bs_vector = np.reshape(bs_vector, (-1, 1))\n",
    "            ss_scalar = np.array([[np.sum(ss)]])\n",
    "    \n",
    "            hessian = np.block([[bb_matrix, bs_vector],\n",
    "                            [bs_vector.T, ss_scalar]])\n",
    "        else:\n",
    "            exp_argument = norm.logpdf(sign * (bX - point / (sigma + epsilon))) - norm.logcdf(sign * (bX - point / (sigma + epsilon)))\n",
    "            exp_argument = np.clip(exp_argument, a_min=None, a_max=700)\n",
    "            mills = np.exp(exp_argument)\n",
    "            \n",
    "            lnL = -np.log(sigma + epsilon) + norm.logpdf(y / (sigma + epsilon) - bX) - norm.logcdf(sign * (bX - point / (sigma + epsilon)))\n",
    "            logLik = lnL.sum() if isinstance(lnL, np.ndarray) and lnL.ndim > 0 else lnL\n",
    "            \n",
    "            #Gradient calculation\n",
    "            gbX = (y / sigma - bX + epsilon) - mills * sign\n",
    "            gsigma = -1 / (sigma + epsilon) + (y / sigma + epsilon - bX) * y /(sigma + epsilon) ** 2 - sign * mills * point/(sigma + epsilon) ** 2\n",
    "            gradient = np.concatenate([gbX, gsigma])\n",
    "            \n",
    "            #Hessian Calculation\n",
    "            bb = -1 + mills * (mills + sign * (bX - point / (sigma + epsilon)))\n",
    "            bs = -y / sigma ** 2 + (mills + sign * (bX - point / (sigma + epsilon))) * mills * point / (sigma + epsilon) ** 2\n",
    "            ss = 1 / (sigma + epsilon) ** 2 - 3 * y ** 2 / (sigma + epsilon) ** 4 + 2 * bX * y / (sigma + epsilon) ** 3 + \\\n",
    "                mills * (mills + sign * (bX - point / (sigma + epsilon))) * point ** 2 / (sigma + epsilon) ** 4 + \\\n",
    "                2 * sign * mills * point / (sigma + epsilon) ** 3\n",
    "            bs = np.array(bs)\n",
    "            \n",
    "            bb_matrix = np.dot(x.T, np.reshape(bb, (-1, 1)) * x)\n",
    "            bs_vector = np.sum(np.reshape(bs, (-1, 1)) * x, axis=0)\n",
    "            bs_vector = np.reshape(bs_vector, (-1, 1))\n",
    "            ss_scalar = np.array([[np.sum(ss)]])\n",
    "            \n",
    "            hessian = np.block([[bb_matrix, bs_vector],\n",
    "                            [bs_vector.T, ss_scalar]])\n",
    "            \n",
    "        return {\n",
    "            'logLik': logLik, \n",
    "            'gradient': gradient, \n",
    "            'hessian': hessian\n",
    "        }\n",
    "\n",
    "    def objective(param):\n",
    "        result = maxLikTruncreg(param, x, y, point, direction)\n",
    "        return -result['logLik']\n",
    "\n",
    "    linmod = np.linalg.lstsq(x, y, rcond=None)\n",
    "    start_beta = linmod[0]\n",
    "    start_sigma = np.array([np.std(linmod[1])])\n",
    "    start = np.concatenate([start_beta, start_sigma])\n",
    "\n",
    "    lower_bounds = [np.NINF] * (len(start) - 1) + [1e-4]\n",
    "    upper_bounds = [np.inf] * len(start)\n",
    "    bounds = Bounds(lower_bounds, upper_bounds)\n",
    "\n",
    "    result = minimize(objective, start, method='L-BFGS-B', bounds=bounds, options={'maxiter': iterlim})\n",
    "    opt_result = maxLikTruncreg(result.x, x, y, point, direction, scaled)\n",
    "    vcov = -np.linalg.inv(opt_result['hessian'])\n",
    "    std_errors = np.sqrt(np.diag(vcov))\n",
    "    \n",
    "    return {\n",
    "        'result': result,\n",
    "        'vcov': vcov,\n",
    "        'SE': std_errors\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result':   message: STOP: TOTAL NO. of ITERATIONS REACHED LIMIT\n",
      "  success: False\n",
      "   status: 1\n",
      "      fun: 2108.38559242432\n",
      "        x: [ 9.393e+02 -4.677e+02 -9.946e+01 -1.575e+01  5.119e+01\n",
      "             5.322e+02]\n",
      "      nit: 50\n",
      "      jac: [-1.723e-02  3.402e-02  2.765e-02 -1.001e+00 -2.767e-01\n",
      "            -9.420e-01]\n",
      "     nfev: 455\n",
      "     njev: 65\n",
      " hess_inv: <6x6 LbfgsInvHessProduct with dtype=float64>, 'vcov': array([[ 1.43858120e+05, -1.25420225e+04, -6.10188647e+03,\n",
      "        -1.73430874e+03, -4.60015031e+03, -1.07771422e+02],\n",
      "       [-1.25420225e+04,  1.14460624e+04,  4.09661983e+02,\n",
      "         2.69847936e+02, -1.29942821e+02,  1.01826624e+02],\n",
      "       [-6.10188647e+03,  4.09661983e+02,  1.26477413e+03,\n",
      "         9.35012849e+01,  3.27066443e+01,  5.75536589e+00],\n",
      "       [-1.73430874e+03,  2.69847936e+02,  9.35012849e+01,\n",
      "         3.54375023e+01,  3.49471335e+00, -6.66373172e-01],\n",
      "       [-4.60015031e+03, -1.29942821e+02,  3.27066443e+01,\n",
      "         3.49471335e+00,  3.54336606e+02, -3.29834148e+00],\n",
      "       [-1.07771422e+02,  1.01826624e+02,  5.75536589e+00,\n",
      "        -6.66373172e-01, -3.29834148e+00,  1.67655274e+02]]), 'SE': array([379.28632962, 106.98627213,  35.56366307,   5.95294065,\n",
      "        18.82383081,  12.94817648])}\n"
     ]
    }
   ],
   "source": [
    "result = truncreg(formula='whrs~kl6+k618+wa+we', data=df, point=0, direction='left')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result':   message: STOP: TOTAL NO. of ITERATIONS REACHED LIMIT\n",
       "   success: False\n",
       "    status: 1\n",
       "       fun: 2108.38559242432\n",
       "         x: [ 9.393e+02 -4.677e+02 -9.946e+01 -1.575e+01  5.119e+01\n",
       "              5.322e+02]\n",
       "       nit: 50\n",
       "       jac: [-1.723e-02  3.402e-02  2.765e-02 -1.001e+00 -2.767e-01\n",
       "             -9.420e-01]\n",
       "      nfev: 455\n",
       "      njev: 65\n",
       "  hess_inv: <6x6 LbfgsInvHessProduct with dtype=float64>,\n",
       " 'vcov': array([[ 1.43858120e+05, -1.25420225e+04, -6.10188647e+03,\n",
       "         -1.73430874e+03, -4.60015031e+03, -1.07771422e+02],\n",
       "        [-1.25420225e+04,  1.14460624e+04,  4.09661983e+02,\n",
       "          2.69847936e+02, -1.29942821e+02,  1.01826624e+02],\n",
       "        [-6.10188647e+03,  4.09661983e+02,  1.26477413e+03,\n",
       "          9.35012849e+01,  3.27066443e+01,  5.75536589e+00],\n",
       "        [-1.73430874e+03,  2.69847936e+02,  9.35012849e+01,\n",
       "          3.54375023e+01,  3.49471335e+00, -6.66373172e-01],\n",
       "        [-4.60015031e+03, -1.29942821e+02,  3.27066443e+01,\n",
       "          3.49471335e+00,  3.54336606e+02, -3.29834148e+00],\n",
       "        [-1.07771422e+02,  1.01826624e+02,  5.75536589e+00,\n",
       "         -6.66373172e-01, -3.29834148e+00,  1.67655274e+02]]),\n",
       " 'SE': array([379.28632962, 106.98627213,  35.56366307,   5.95294065,\n",
       "         18.82383081,  12.94817648])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import log_ndtr\n",
    "from scipy.stats import norm\n",
    "from statsmodels.api import OLS\n",
    "from statsmodels.regression.linear_model import (\n",
    "    OLSResults, # noqa\n",
    "    RegressionResultsWrapper,\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Tobit(OLS):\n",
    "    def __init__(\n",
    "        self,\n",
    "        endog,\n",
    "        exog=None,\n",
    "        reparam=True,\n",
    "        c_lw=0.0,\n",
    "        c_up=None,\n",
    "        ols_option=True,\n",
    "        missing=\"none\",\n",
    "        hasconst=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param endog: np.ndarray or pd.Series or pd.DataFrame - endogenous variable\n",
    "        :param exog: np.ndarray or pd.Series - exogenous variable(s)\n",
    "        :param reparam: bool - specifies whether to use Olsen's reparameterization\n",
    "        :param c_lw: int or float or floating - lower censoring limit\n",
    "        :param c_up: int or float or floating - upper censoring limit\n",
    "        :param ols_option: bool - specifies whether to use OLS analytical\n",
    "        solution or MLE when no threshold value is given\n",
    "        :param missing: see OLS documentation\n",
    "        :param hasconst: see OLS documentation\n",
    "        :param kwargs: see OLS documentation\n",
    "        \"\"\"\n",
    "        self._c_lw = c_lw\n",
    "        self._c_up = c_up\n",
    "        self.reparam = reparam\n",
    "        self.scale = None\n",
    "        self.params = None\n",
    "        self.ols_params = None\n",
    "        self.ols_option = ols_option\n",
    "\n",
    "        endog_copy = copy.deepcopy(endog)\n",
    "        exog_copy = copy.deepcopy(exog)\n",
    "\n",
    "        super().__init__(endog_copy, exog_copy, missing=missing, hasconst=hasconst, **kwargs)\n",
    "        if isinstance(endog, (pd.DataFrame, pd.Series)):\n",
    "            endog = endog.values\n",
    "        if isinstance(exog, pd.DataFrame):\n",
    "            exog = exog.values\n",
    "        if c_lw is not None and c_up is None:\n",
    "            self.left_endog = endog[endog == c_lw]\n",
    "            self.left_exog = exog[endog == c_lw, :]\n",
    "            self.free_endog = endog[endog > c_lw]\n",
    "            self.free_exog = exog[endog > c_lw, :]\n",
    "        elif c_lw is None and c_up is not None:\n",
    "            self.free_endog = endog[endog < c_up]\n",
    "            self.free_exog = exog[endog < c_up, :]\n",
    "            self.right_endog = endog[endog == c_up]\n",
    "            self.right_exog = exog[endog == c_up, :]\n",
    "        elif c_lw is not None and c_up is not None:\n",
    "            self.left_endog = endog[endog == c_lw]\n",
    "            self.left_exog = exog[endog == c_lw]\n",
    "            self.free_endog = endog[(endog > c_lw) & (endog < c_up)]\n",
    "            self.free_exog = exog[\n",
    "                (endog > c_lw) & (endog < c_up), :\n",
    "            ]\n",
    "            self.right_endog = endog[endog == c_up]\n",
    "            self.right_exog = exog[endog == c_up]\n",
    "        else:\n",
    "            self.free_endog = endog\n",
    "            self.free_exog = exog\n",
    "            \n",
    "            logger.info(\n",
    "                \"No censoring threshold provided; OLS will be used for model estimation.\"\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def c_lw(self):\n",
    "        return self._c_lw\n",
    "\n",
    "    @property\n",
    "    def c_up(self):\n",
    "        return self._c_up\n",
    "\n",
    "    @c_lw.setter\n",
    "    def c_lw(self, new_value):\n",
    "        raise AttributeError(\"Cannot modify lower threshold value after it is set.\")\n",
    "\n",
    "    @c_up.setter\n",
    "    def c_up(self, new_value):\n",
    "        raise AttributeError(\"Cannot modify upper threshold value after it is set.\")\n",
    "\n",
    "    def neg_llh_jac(self, params):\n",
    "        scale, betas = params[0], params[1:]\n",
    "\n",
    "        scale_jac_censored = 0\n",
    "        betas_jac_censored = np.zeros(len(betas))\n",
    "        if self._c_lw is not None and self.left_endog.size:\n",
    "            left_zscores = (self.left_endog - np.dot(self.left_exog, betas)) / scale\n",
    "            left_d_dscale = np.dot(\n",
    "                norm.pdf(left_zscores) / norm.cdf(left_zscores), -left_zscores / scale\n",
    "            )\n",
    "            left_d_dbetas = np.dot(\n",
    "                norm.pdf(left_zscores) / norm.cdf(left_zscores), -self.left_exog / scale\n",
    "            )\n",
    "            scale_jac_censored += left_d_dscale\n",
    "            betas_jac_censored += left_d_dbetas\n",
    "\n",
    "        if self._c_up is not None and self.right_endog.size:\n",
    "            right_zscores = (np.dot(self.right_exog, betas) - self.right_endog) / scale\n",
    "            right_d_dscale = np.dot(\n",
    "                norm.pdf(right_zscores) / norm.cdf(right_zscores),\n",
    "                -right_zscores / scale,\n",
    "            )\n",
    "            right_d_dbetas = np.dot(\n",
    "                norm.pdf(right_zscores) / norm.cdf(right_zscores),\n",
    "                self.right_exog / scale,\n",
    "            )\n",
    "            scale_jac_censored += right_d_dscale\n",
    "            betas_jac_censored += right_d_dbetas\n",
    "\n",
    "        free_zscores = (self.free_endog - np.dot(self.free_exog, betas)) / scale\n",
    "        free_d_dscale = np.sum(free_zscores**2 - 1) / scale\n",
    "        free_d_dbetas = np.dot(free_zscores, self.free_exog / scale)\n",
    "\n",
    "        scale_jac = scale_jac_censored + free_d_dscale\n",
    "        betas_jac = betas_jac_censored + free_d_dbetas\n",
    "\n",
    "        return -np.append(scale_jac, betas_jac)\n",
    "\n",
    "    def loglike(self, params):  # noqa\n",
    "        if hasattr(self, \"llh\"):\n",
    "            return self.llh\n",
    "        else:\n",
    "            return super().loglike(params)\n",
    "\n",
    "    def neg_llh_func(self, params):\n",
    "        scale, betas = params[0], params[1:]\n",
    "\n",
    "        llf_censored = 0\n",
    "\n",
    "        if self._c_lw is not None and self.left_endog.size:\n",
    "            llf_left = np.sum(\n",
    "                log_ndtr((self.left_endog - np.dot(self.left_exog, betas)) / scale)\n",
    "            )\n",
    "            llf_censored += llf_left\n",
    "        if self._c_up is not None and self.right_endog.size:\n",
    "            llf_right = np.sum(\n",
    "                log_ndtr((np.dot(self.right_exog, betas) - self.right_endog) / scale)\n",
    "            )\n",
    "            llf_censored += llf_right\n",
    "\n",
    "        llf_free = np.sum(\n",
    "            norm.logpdf((self.free_endog - np.dot(self.free_exog, betas)) / scale)\n",
    "            - np.log(max(scale, np.finfo(\"float\").resolution))\n",
    "        )\n",
    "\n",
    "        return -1 * (llf_censored + llf_free)\n",
    "\n",
    "    # Reparameterization\n",
    "    def neg_llh_func2(self, params):\n",
    "        gamma, delta = params[0], params[1:]\n",
    "\n",
    "        llf_censored = 0\n",
    "        if self._c_lw is not None and self.left_endog.size:\n",
    "            llf_left = np.sum(\n",
    "                log_ndtr(gamma * self.left_endog - np.dot(self.left_exog, delta))\n",
    "            )\n",
    "            llf_censored += llf_left\n",
    "        if self._c_up is not None and self.right_endog.size:\n",
    "            llf_right = np.sum(\n",
    "                log_ndtr(np.dot(self.right_exog, delta) - gamma * self.right_endog)\n",
    "            )\n",
    "            llf_censored += llf_right\n",
    "\n",
    "        llf_free = np.sum(\n",
    "            np.log(max(gamma, np.finfo(\"float\").resolution))\n",
    "            + norm.logpdf(gamma * self.free_endog - np.dot(self.free_exog, delta))\n",
    "        )\n",
    "\n",
    "        return -1 * (llf_censored + llf_free)\n",
    "\n",
    "    def neg_llh_jac2(self, params):\n",
    "        gamma, delta = params[0], params[1:]\n",
    "\n",
    "        gamma_jac_censored = 0\n",
    "        delta_jac_censored = np.zeros(len(delta))\n",
    "\n",
    "        if self._c_lw is not None and self.left_endog.size:\n",
    "            left_zscore = gamma * self.left_endog - np.dot(self.left_exog, delta)\n",
    "            left_d_dgamma = np.dot(\n",
    "                norm.pdf(left_zscore) / norm.cdf(left_zscore), self.left_endog\n",
    "            )\n",
    "            left_d_ddelta = np.dot(\n",
    "                norm.pdf(left_zscore) / norm.cdf(left_zscore), -self.left_exog\n",
    "            )\n",
    "            delta_jac_censored += left_d_ddelta\n",
    "            gamma_jac_censored += left_d_dgamma\n",
    "\n",
    "        if self._c_up is not None and self.right_endog.size:\n",
    "            right_zscore = np.dot(self.right_exog, delta) - gamma * self.right_endog\n",
    "            right_d_dgamma = np.dot(\n",
    "                norm.pdf(right_zscore) / norm.cdf(right_zscore), -self.right_endog\n",
    "            )\n",
    "            right_d_ddelta = np.dot(\n",
    "                norm.pdf(right_zscore) / norm.cdf(right_zscore), self.right_exog\n",
    "            )\n",
    "            delta_jac_censored += right_d_ddelta\n",
    "            gamma_jac_censored += right_d_dgamma\n",
    "\n",
    "        free_zscore = gamma * self.free_endog - np.dot(self.free_exog, delta)\n",
    "        free_d_dgamma = np.sum(1 / gamma - free_zscore * self.free_endog)\n",
    "        free_d_ddelta = np.dot(free_zscore, self.free_exog)\n",
    "\n",
    "        gamma_jac = gamma_jac_censored + free_d_dgamma\n",
    "        delta_jac = delta_jac_censored + free_d_ddelta\n",
    "\n",
    "        return -np.append(gamma_jac, delta_jac)\n",
    "\n",
    "    def fit_tobit(self, cov_type=\"HC1\", cov_kwds=None, use_t=None, verbose=True):\n",
    "        modl = super().fit(cov_type=cov_type)  # noqa\n",
    "        self.ols_params = copy.deepcopy(modl.params)\n",
    "        self.scale = np.sqrt(np.cov(modl.resid))  # noqa\n",
    "\n",
    "        self.run_optimize(verbose)\n",
    "\n",
    "        lfit = OLSResults(\n",
    "            self,\n",
    "            self.params,\n",
    "            normalized_cov_params=self.normalized_cov_params,\n",
    "            scale=self.scale,\n",
    "            cov_type=cov_type,\n",
    "            cov_kwds=cov_kwds,\n",
    "            use_t=use_t,\n",
    "        )\n",
    "\n",
    "        return RegressionResultsWrapper(lfit)\n",
    "\n",
    "    def fit(self, cov_type=\"HC1\", cov_kwds=None, use_t=None, verbose=False, **kwargs):\n",
    "        if (\n",
    "            self._c_lw is None\n",
    "            and self._c_up is None\n",
    "            and self.ols_option\n",
    "        ):\n",
    "            return super().fit(cov_type=cov_type, cov_kwds=None, use_t=None, **kwargs)\n",
    "        else:\n",
    "            return self.fit_tobit(\n",
    "                cov_type=cov_type, cov_kwds=None, use_t=None, verbose=verbose\n",
    "            )\n",
    "\n",
    "    def run_optimize(self, verbose):\n",
    "        initial_params, func, jac = self.get_initial_params_and_functions()\n",
    "        result = minimize(\n",
    "            func,\n",
    "            initial_params,\n",
    "            method=\"BFGS\",\n",
    "            jac=jac,\n",
    "            options={\"disp\": verbose},\n",
    "        )\n",
    "        self.update_model_parameters(result)\n",
    "        if verbose:\n",
    "            logger.info(result)\n",
    "\n",
    "    def get_initial_params_and_functions(self):\n",
    "        if self.reparam:\n",
    "            initial_params = np.append(1 / self.scale, self.ols_params / self.scale)\n",
    "            func = self.neg_llh_func2\n",
    "            jac = self.neg_llh_jac2\n",
    "        else:\n",
    "            initial_params = np.append(self.scale, self.ols_params)\n",
    "            func = self.neg_llh_func\n",
    "            jac = self.neg_llh_jac\n",
    "        return initial_params, func, jac\n",
    "\n",
    "    def update_model_parameters(self, result):\n",
    "        self.llh = -result.fun  # noqa\n",
    "        if self.reparam:\n",
    "            self.scale = 1 / result.x[0]\n",
    "            self.params = result.x[1:] * self.scale\n",
    "        else:\n",
    "            self.scale = result.x[0]\n",
    "            self.params = result.x[1:]\n",
    "        self.normalized_cov_params = result.hess_inv[1:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:726: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n",
      "INFO:__main__:  message: Desired error not necessarily achieved due to precision loss.\n",
      "  success: False\n",
      "   status: 2\n",
      "      fun: 1367.333919287908\n",
      "        x: [ 7.606e-04 -5.937e-01 -8.880e-02 -1.361e-02  9.326e-02]\n",
      "      nit: 13\n",
      "      jac: [ 3.048e-04  8.574e-07  1.390e-06 -7.101e-06  1.386e-06]\n",
      " hess_inv: [[ 2.300e-09 -8.432e-07 ...  3.424e-09  1.817e-07]\n",
      "            [-8.432e-07  2.384e-02 ...  3.278e-04 -1.528e-03]\n",
      "            ...\n",
      "            [ 3.424e-09  3.278e-04 ...  4.090e-05 -1.468e-04]\n",
      "            [ 1.817e-07 -1.528e-03 ... -1.468e-04  6.009e-04]]\n",
      "     nfev: 20\n",
      "     njev: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: 1367.333919\n",
      "         Iterations: 13\n",
      "         Function evaluations: 20\n",
      "         Gradient evaluations: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<statsmodels.regression.linear_model.RegressionResultsWrapper at 0x1e5b48ef410>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "y = df[\"whrs\"]\n",
    "X = df[[\"kl6\", \"k618\", \"wa\", \"we\"]]\n",
    "X = sm.add_constant(X)\n",
    "model = Tobit(df[\"whrs\"], df[[\"kl6\", \"k618\", \"wa\", \"we\"]], c_lw=0)\n",
    "model.fit(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:317: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:317: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20364\\1168873673.py:317: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import enum\n",
    "from scipy.stats import norm\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "@enum.unique\n",
    "class TobitType(enum.IntEnum):\n",
    "    \"\"\" Type of Tobit Regression Model \"\"\"\n",
    "    TYPE1 = 1,\n",
    "    TYPE2 = 2,\n",
    "    TYPE3 = 3\n",
    "\n",
    "\n",
    "class RegressionResult(object):\n",
    "    \"\"\" Regression results class \"\"\"\n",
    "    def __init__(self):\n",
    "        self._history = []\n",
    "        self._params = None\n",
    "        self._vars = None\n",
    "        self._fittedModel = None\n",
    "\n",
    "    @property\n",
    "    def history(self):\n",
    "        return self._history\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return self._params\n",
    "\n",
    "    @parameters.setter\n",
    "    def parameters(self, value):\n",
    "        self._params = value\n",
    "        if len(value) == 2:\n",
    "            self._params = value[0]\n",
    "\n",
    "    @property\n",
    "    def varnames(self):\n",
    "        return self._vars\n",
    "\n",
    "    @varnames.setter\n",
    "    def varnames(self, value):\n",
    "        self._vars = value\n",
    "\n",
    "    def setModel(self, fitted_model):\n",
    "        self._fittedModel = fitted_model\n",
    "\n",
    "    def appendDiff(self, diff):\n",
    "        \"\"\"\n",
    "        Append parameter diff across iterations to history\n",
    "        :param diff: mean sum of square diff in model parameters across iterations\n",
    "        \"\"\"\n",
    "        self._history.append(diff)\n",
    "\n",
    "    def predict(self, exog: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the model output. Model must have been fitted to data\n",
    "        :param exog: exogeneous variables (X)\n",
    "        :return: model output (y)\n",
    "        \"\"\"\n",
    "        return self._fittedModel._predict(exog, self._params)\n",
    "\n",
    "\n",
    "class TobitRegression(object):\n",
    "    \"\"\" Tobit (censored) regression model \"\"\"\n",
    "    DEFAULT_DTYPE = np.float32\n",
    "\n",
    "    def __init__(self, low=None, high=None, type=TobitType.TYPE1, diff_thresh=1E-10, niters=1000, dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Initialize the regression model\n",
    "        :param low: Low threshold for censoring\n",
    "        :param high: High threshold for censoring\n",
    "        :param type: Tobit type 1, 2 or 3. Currently only type 1 is supported. Future releases will add support for\n",
    "        type 2 and type 3 Tobit regressions\n",
    "        :param diff_thresh: Stop iterations when diff between successive iterations becomes lower than this threshold\n",
    "        :param niters: Maximum number of iterations\n",
    "        :param dtype: data type of numeric variables. Default is 32 bit float\n",
    "        \"\"\"\n",
    "        assert (low is not None) or (high is not None), \"both low and high cannot be None\"\n",
    "        if (low is not None) and (high is not None):\n",
    "            assert low < high, \"low must be strictly less than high\"\n",
    "        self.DEFAULT_DTYPE = dtype\n",
    "        self.diffThreshold = diff_thresh\n",
    "        self.maxIters = niters\n",
    "        self.delta = None\n",
    "        self.gamma = None\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        self.aIndex = np.array([])\n",
    "        self.bIndex = np.array([])\n",
    "        self.midIndex = np.array([])\n",
    "        self.funcVal = None\n",
    "        self.funcDeriv = None\n",
    "        self.categoricalMap = {}\n",
    "        self.colNames = []\n",
    "        self.categoricalColInd = None\n",
    "        self.hasConst = None\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return self.delta/self.gamma, 1.0/self.gamma\n",
    "\n",
    "    def initializeParams(self, nexog: int, endog: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Initialize model parameters\n",
    "        :param nexog: number of exogeneous (independent) variables\n",
    "        :param endog: endogeneous (dependent) variable array\n",
    "        \"\"\"\n",
    "        self.delta = np.ones(nexog, dtype=self.DEFAULT_DTYPE)\n",
    "        self.funcVal = np.zeros(nexog + 1, dtype=self.DEFAULT_DTYPE)\n",
    "        self.funcDeriv = np.zeros((nexog + 1, nexog + 1), dtype=self.DEFAULT_DTYPE)\n",
    "        self.gamma = 1.0\n",
    "        indicA = 0\n",
    "        indicB = 0\n",
    "        if self.low is not None:\n",
    "            indicA = np.less_equal(endog, self.low)\n",
    "            if indicA.sum():\n",
    "                self.aIndex = np.where(indicA)[0]\n",
    "        if self.high is not None:\n",
    "            indicB = np.greater_equal(endog, self.high)\n",
    "            if indicB.sum():\n",
    "                self.bIndex = np.where(indicB)[0]\n",
    "        self.midIndex = np.where(1 - indicA - indicB)[0]\n",
    "\n",
    "    def lowLikelihood(self, exog: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate log likelihood of output value at lower threshold\n",
    "        :param exog: Array of exogeneous variables\n",
    "        :return: Log likelihood of output at lower threshold\n",
    "        \"\"\"\n",
    "        required = self.aIndex\n",
    "        xdelta = self.low * self.gamma - np.einsum(\"ij,j->i\", exog[required, :], self.delta)\n",
    "        cdf = norm.cdf(xdelta)\n",
    "        return np.log(cdf).sum()\n",
    "\n",
    "    def highLikelihood(self, exog: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate log likelihood of output value at higher threshold\n",
    "        :param exog: Array of exogeneous variables\n",
    "        :return: Log likelihood of output value at lower threshold\n",
    "        \"\"\"\n",
    "        required = self.bIndex\n",
    "        xdelta = -self.high * self.gamma + np.einsum(\"ij,j->i\", exog[required, :], self.delta)\n",
    "        cdf = norm.cdf(xdelta)\n",
    "        return np.log(cdf).sum()\n",
    "\n",
    "    def midLikelihood(self, exog: np.ndarray, endog: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate log likelihood of output value not hitting lower or higher thresholds (in uncensored region)\n",
    "        :param exog: Array of exogeneous variables\n",
    "        :param endog: Array of endogeneous (output) variables\n",
    "        :return: log likelihood of output value in uncensored region\n",
    "        \"\"\"\n",
    "        required = self.midIndex\n",
    "        xdelta = self.gamma * endog[required] - np.einsum(\"ij,j->i\", exog[required, :], self.delta)\n",
    "        pdf = norm.pdf(xdelta)\n",
    "        return np.log(pdf * self.gamma).sum()\n",
    "\n",
    "    def logLikelihood(self, exog: np.ndarray, endog: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Log likelihood of data subject to lower and higher thresholds (censoring)\n",
    "        :param exog: Array of exogeneous variables\n",
    "        :param endog: Array of endogeneous variables\n",
    "        :return: Log likelihood of data subject to lower and higher thresholds\n",
    "        \"\"\"\n",
    "        result = 0\n",
    "        if self.low is not None:\n",
    "            result += self.lowLikelihood(exog)\n",
    "        if self.high is not None:\n",
    "            result += self.highLikelihood(exog)\n",
    "\n",
    "        result += self.midLikelihood(exog, endog)\n",
    "        return result\n",
    "\n",
    "    def lowFunc(self, exog: np.ndarray, val: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Function (= derivative of log likelihood) at lower censoring threshold\n",
    "        :param exog: array of exogeneous variables\n",
    "        :param val: function value at lower threshold\n",
    "        \"\"\"\n",
    "        required = self.aIndex\n",
    "        xdelta = self.low * self.gamma - np.einsum(\"ij,j->i\", exog[required, :], self.delta)\n",
    "        den = norm.cdf(xdelta)\n",
    "        num = norm.pdf(xdelta)\n",
    "        limindex = (den == 0.) & (num == 0.)\n",
    "        den = np.where(limindex, 1.0, den)\n",
    "        num = np.where(limindex, 1.0, num)\n",
    "        frac = num / den\n",
    "        val[0:-1] += -np.einsum(\"i,ij->j\", frac, exog[required, :])\n",
    "        val[-1] += frac.sum() * self.low\n",
    "\n",
    "    def highFunc(self, exog: np.ndarray, val: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Function at higher censoring threshold\n",
    "        :param exog: array of exogeneous variables\n",
    "        :param val: value of function at upper censoring threshold\n",
    "        \"\"\"\n",
    "        required = self.bIndex\n",
    "        xdelta = -self.high * self.gamma + np.einsum(\"ij,j->i\", exog[required, :], self.delta)\n",
    "        den = norm.cdf(xdelta)\n",
    "        num = norm.pdf(xdelta)\n",
    "        limindex = (den == 0.) & (num == 0.)\n",
    "        den = np.where(limindex, 1.0, den)\n",
    "        num = np.where(limindex, 1.0, num)\n",
    "        frac = num / den\n",
    "        val[0:-1] += np.einsum(\"i,ij->j\", frac, exog[required, :])\n",
    "        val[-1] += -frac.sum() * self.high\n",
    "\n",
    "    def midFunc(self, exog: np.ndarray, endog: np.ndarray, val: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Function value in uncensored region\n",
    "        :param exog: array of exogeneous variables\n",
    "        :param endog: array of output variables\n",
    "        :param val: value of function\n",
    "        \"\"\"\n",
    "        required = self.midIndex\n",
    "        xdelta = self.gamma * endog[required] - np.einsum(\"ij,j->i\", exog[required, :], self.delta)\n",
    "        val[0:-1] += np.einsum(\"i,ij->j\", xdelta, exog[required, :])\n",
    "        val[-1] += 1.0 / self.gamma - np.multiply(xdelta, endog[required]).sum()\n",
    "\n",
    "    def function(self, exog: np.ndarray, endog: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Function (derivative of log likelihood) that we are trying to determine a root of\n",
    "        :param exog: array of exogeneous variables (X)\n",
    "        :param endog: array of output variables (y)\n",
    "        :return: array of function values evaluated at specified X and y\n",
    "        \"\"\"\n",
    "        val = self.funcVal\n",
    "        val[:] = 0\n",
    "        if self.aIndex.shape[0]:\n",
    "            self.lowFunc(exog, val)\n",
    "        if self.bIndex.shape[0]:\n",
    "            self.highFunc(exog, val)\n",
    "        self.midFunc(exog, endog, val)\n",
    "        return val\n",
    "\n",
    "    def lowDeriv(self, exog: np.ndarray, val: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Derivative of function (which was derivative of log likelihood) at lower censoring threshold\n",
    "        :param exog: array of exogeneous variables\n",
    "        :param val: Derivative of function (2 dimensional array) at lower censoring threshold\n",
    "        \"\"\"\n",
    "        required = self.aIndex\n",
    "        xdelta = self.low * self.gamma - np.einsum(\"ij,j->i\", exog[required, :], self.delta)\n",
    "        den = norm.cdf(xdelta)\n",
    "        num = norm.pdf(xdelta)\n",
    "        limindex = (den == 0.) & (num == 0.)\n",
    "        den = np.where(limindex, 1.0, den)\n",
    "        num = np.where(limindex, 1.0, num)\n",
    "        frac = num / den\n",
    "        termfrac = np.multiply(frac, 1 - frac)\n",
    "        prod = np.multiply(frac, xdelta)\n",
    "        termprod = np.multiply(prod, 1 - prod)\n",
    "        val[0:-1, 0:-1] += np.einsum(\"i,ij,ik->jk\", termprod, exog[required, :], exog[required, :])\n",
    "        val[-1, -1] += termfrac.sum() * self.low * self.low\n",
    "        crossDeriv = -self.low * np.einsum(\"i,ij->j\", termfrac, exog[required, :])\n",
    "        val[-1, 0:-1] += crossDeriv\n",
    "        val[0:-1, -1] += crossDeriv\n",
    "\n",
    "    def highDeriv(self, exog: np.ndarray, val: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "         Derivative of function (which was derivative of log likelihood) at upper censoring threshold\n",
    "        :param exog: array of exogeneous variables\n",
    "        :param val: Derivative of function (2 dimensional array) at uppper censoring threshold\n",
    "        \"\"\"\n",
    "        required = self.bIndex\n",
    "        xdelta = -self.high * self.gamma + np.einsum(\"ij,j->i\", exog[required, :], self.delta)\n",
    "        den = norm.cdf(xdelta)\n",
    "        num = norm.pdf(xdelta)\n",
    "        limindex = (den == 0.) & (num == 0.)\n",
    "        den = np.where(limindex, 1.0, den)\n",
    "        num = np.where(limindex, 1.0, num)\n",
    "        frac = num / den\n",
    "        termfrac = np.multiply(frac, 1 - frac)\n",
    "        prod = np.multiply(frac, xdelta)\n",
    "        termprod = np.multiply(prod, 1 - prod)\n",
    "        val[0:-1, 0:-1] += np.einsum(\"i,ij,ik->jk\", termprod, exog[required, :], exog[required, :])\n",
    "        val[-1, -1] += termfrac.sum() * self.high * self.high\n",
    "        crossDeriv = -self.high * np.einsum(\"i,ij->j\", termfrac, exog[required, :])\n",
    "        val[-1, 0:-1] += crossDeriv\n",
    "        val[0:-1, -1] += crossDeriv\n",
    "\n",
    "    def midDeriv(self, exog: np.ndarray, endog: np.ndarray, val: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Derivative of function (which was derivative of log likelihood) in uncensored region\n",
    "        :param exog: array of exogeneous variables\n",
    "        :param endog: array of output variables\n",
    "        :param val: Derivative of function (2 dimensional array) in uncensoring region\n",
    "        \"\"\"\n",
    "        required = self.midIndex\n",
    "        val[0:-1, 0:-1] += -np.einsum(\"ij,ik->jk\", exog[required, :], exog[required, :])\n",
    "        val[-1, -1] += -(1.0 / (self.gamma * self.gamma) +\n",
    "                         np.multiply(endog[required], endog[required]).sum())\n",
    "        yixij = np.einsum(\"i,ij->j\", endog[required], exog[required, :])\n",
    "        val[-1, 0:-1] += yixij\n",
    "        val[0:-1, -1] += yixij\n",
    "\n",
    "    def derivative(self, exog: np.ndarray, endog: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Derivative of function (which was derivative of log likelihood function)\n",
    "        :param exog: array of exogeneous variables\n",
    "        :param endog: array of output variables\n",
    "        :return: Derivative (2 dimensional array) of function. Function is the derivative of log likelihood\n",
    "        \"\"\"\n",
    "        deriv = self.funcDeriv\n",
    "        deriv[:, :] = 0\n",
    "        if self.aIndex.shape[0]:\n",
    "            self.lowDeriv(exog, deriv)\n",
    "        if self.bIndex.shape[0]:\n",
    "            self.highDeriv(exog, deriv)\n",
    "        self.midDeriv(exog, endog, deriv)\n",
    "        return deriv\n",
    "\n",
    "    def newtonStep(self, exog: np.ndarray, endog: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Perform a Newton step to update model parameters. \\delta x = - f(x) / gradient(f(x))\n",
    "        :param exog: array of exogeneous variables\n",
    "        :param endog: array of output variables\n",
    "        :return: diff or the change in parameters\n",
    "        \"\"\"\n",
    "        fx = self.function(exog, endog)\n",
    "        jacobian = self.derivative(exog, endog)\n",
    "        step = np.linalg.solve(jacobian, -fx)\n",
    "        self.delta += step[0:-1]\n",
    "        self.gamma += step[-1]\n",
    "        self.gamma = abs(self.gamma)\n",
    "        return np.multiply(step, step).sum() / step.shape[0]\n",
    "\n",
    "    def coerceData(self, exog: np.ndarray, endog: np.ndarray, categorical: tuple,\n",
    "                   col_names: tuple) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Coerce data to default datatype\n",
    "        :param exog:\n",
    "        :param endog:\n",
    "        :param categorical:\n",
    "        :param col_names:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.categoricalMap = {}\n",
    "        catset = set(categorical)\n",
    "        noncat = [i for i in range(exog.shape[1]) if i not in catset]\n",
    "        colnames = [col_names[i] for i in range(exog.shape[1]) if i not in catset]\n",
    "        concatarr = [exog[:, noncat]]\n",
    "        for cat in categorical:\n",
    "            distinct = sorted(list(set(exog[:, cat])))\n",
    "            self.categoricalMap[cat] = distinct\n",
    "            catcols = np.zeros((exog.shape[0], len(distinct)-1), dtype=self.DEFAULT_DTYPE)\n",
    "            for i, d in enumerate(distinct[0:-1]):\n",
    "                catcols[:, i] = np.where(exog[:, cat] == d, 1, 0)\n",
    "            concatarr.append(catcols)\n",
    "            colnames.extend([\"%s_%s\" % (col_names[cat], si) for si in distinct[0:-1]])\n",
    "        modExog = np.concatenate(concatarr, axis=1)\n",
    "        self.colNames = colnames\n",
    "        return modExog.astype(self.DEFAULT_DTYPE), endog.astype(self.DEFAULT_DTYPE)\n",
    "\n",
    "    def fit(self, exog: np.ndarray, endog: np.ndarray, include_constant: bool = True, categorical: tuple = (),\n",
    "            col_names: tuple = ()) -> RegressionResult:\n",
    "        \"\"\"\n",
    "        Fir the Tobit regression model to the data\n",
    "        :param exog: array of exogeneous variables\n",
    "        :param endog: array of output variables\n",
    "        :param include_constant: Add constant to exogeneous variables array\n",
    "        :param categorical: tuple containing indices of categorical columns\n",
    "        :param col_names: Column names (for ease of identification of fitted parameters)\n",
    "        :return: Regression results\n",
    "        \"\"\"\n",
    "        if not col_names:\n",
    "            col_names = tuple(\"col_%d\" % i for i in range(exog.shape[1]))\n",
    "        self.hasConst = include_constant\n",
    "        if include_constant:\n",
    "            const = np.ones((exog.shape[0], 1), dtype=self.DEFAULT_DTYPE)\n",
    "            exog = np.concatenate((exog, const), axis=1)\n",
    "            col_names = col_names + (\"intercept\",)\n",
    "        self.categoricalColInd = categorical\n",
    "        exog, endog = self.coerceData(exog, endog, categorical, col_names)\n",
    "        self.initializeParams(exog.shape[1], endog)\n",
    "        diff = self.diffThreshold + 1\n",
    "        iters = 0\n",
    "        res = RegressionResult()\n",
    "        while (diff > self.diffThreshold) and (iters < self.maxIters):\n",
    "            diff = self.newtonStep(exog, endog)\n",
    "            iters += 1\n",
    "            res.appendDiff(diff)\n",
    "        self.logger.info(\"Iterations: %d, final residual: %f\", iters, diff)\n",
    "        res.parameters = self.parameters\n",
    "        res.varnames = self.colNames\n",
    "        return res\n",
    "\n",
    "    def _coerceExogForPred(self, exog: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Coerce exogeneous variables array during prediction to handle categorical variables\n",
    "        :param exog: array of exogeneous variables\n",
    "        :return: Processed exogeneous variables with categorical columns converted to indicator variable columns\n",
    "        \"\"\"\n",
    "        catset = set(self.categoricalColInd)\n",
    "        noncat = [i for i in range(exog.shape[1]) if i not in catset]\n",
    "        concatarr = [exog[:, noncat]]\n",
    "        for cat in self.categoricalColInd:\n",
    "            distinct = sorted(list(set(exog[:, cat])))\n",
    "            catcols = np.zeros((exog.shape[0], len(distinct) - 1), dtype=self.DEFAULT_DTYPE)\n",
    "            for i, d in enumerate(distinct[0:-1]):\n",
    "                catcols[:, i] = np.where(exog[:, cat] == d, 1, 0)\n",
    "            concatarr.append(catcols)\n",
    "        modExog = np.concatenate(concatarr, axis=1)\n",
    "        return modExog.astype(self.DEFAULT_DTYPE)\n",
    "\n",
    "    def _predict(self, exog: np.ndarray, params: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the output of the model using exogeneous variables as input. This method should not be called directly.\n",
    "        Use predict method from RegressionResults object\n",
    "        :param exog: exogeneous variables (X)\n",
    "        :param params: fitted model parameters (provided by RegressionResult object)\n",
    "        :return: output value from the model (y)\n",
    "        \"\"\"\n",
    "        if self.hasConst:\n",
    "            exog2 = np.ndarray((exog.shape[0], exog.shape[1] + 1), dtype=self.DEFAULT_DTYPE)\n",
    "            exog2[:, 0:-1] = exog\n",
    "            exog2[:, -1] = 1.0\n",
    "            exog = exog2\n",
    "\n",
    "        exog = self._coerceExogForPred(exog)\n",
    "        vals = np.einsum(\"ij,j->i\", exog, params)\n",
    "        if self.low:\n",
    "            vals = np.where(vals <= self.low, self.low, vals)\n",
    "        if self.high:\n",
    "            vals = np.where(vals > self.high, self.high, vals)\n",
    "        return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_20364\\1168873673.py:317: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m TobitRegression(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mparameters)\n",
      "Cell \u001b[1;32mIn[8], line 383\u001b[0m, in \u001b[0;36mTobitRegression.fit\u001b[1;34m(self, exog, endog, include_constant, categorical, col_names)\u001b[0m\n\u001b[0;32m    381\u001b[0m res \u001b[38;5;241m=\u001b[39m RegressionResult()\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (diff \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffThreshold) \u001b[38;5;129;01mand\u001b[39;00m (iters \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxIters):\n\u001b[1;32m--> 383\u001b[0m     diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewtonStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m     iters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    385\u001b[0m     res\u001b[38;5;241m.\u001b[39mappendDiff(diff)\n",
      "Cell \u001b[1;32mIn[8], line 325\u001b[0m, in \u001b[0;36mTobitRegression.newtonStep\u001b[1;34m(self, exog, endog)\u001b[0m\n\u001b[0;32m    323\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(exog, endog)\n\u001b[0;32m    324\u001b[0m jacobian \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mderivative(exog, endog)\n\u001b[1;32m--> 325\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjacobian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mfx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m step[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m step[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\linalg\\linalg.py:409\u001b[0m, in \u001b[0;36msolve\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m    407\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdd->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    408\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 409\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(r\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\linalg\\linalg.py:112\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "model = TobitRegression(low=0)\n",
    "result = model.fit(X.values, y.values)\n",
    "print(result.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadstat as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 753 entries, 0 to 752\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   lfp               753 non-null    int64  \n",
      " 1   whrs              753 non-null    int64  \n",
      " 2   kl6               753 non-null    int64  \n",
      " 3   k618              753 non-null    int64  \n",
      " 4   wa                753 non-null    int64  \n",
      " 5   we                753 non-null    int64  \n",
      " 6   ww                753 non-null    float64\n",
      " 7   rpwg              753 non-null    float64\n",
      " 8   hhrs              753 non-null    int64  \n",
      " 9   ha                753 non-null    int64  \n",
      " 10  he                753 non-null    int64  \n",
      " 11  hw                753 non-null    float64\n",
      " 12  faminc            753 non-null    int64  \n",
      " 13  mtr               753 non-null    float64\n",
      " 14  wmed              753 non-null    int64  \n",
      " 15  wfed              753 non-null    int64  \n",
      " 16  un                753 non-null    float64\n",
      " 17  cit               753 non-null    int64  \n",
      " 18  ax                753 non-null    int64  \n",
      " 19  salario_ofrecido  753 non-null    float64\n",
      " 20  salhat            753 non-null    float64\n",
      "dtypes: float64(7), int64(14)\n",
      "memory usage: 123.7 KB\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\HP\\OneDrive\\Escritorio\\David Guzzi\\DiTella\\MEC\\Materias\\2024 3T\\[MT06] Microeconometría I\\Clases\\Stata\\MROZ1.dta\"\n",
    "\n",
    "df, meta = st.read_dta(path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.657366\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Probit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>lfp</td>       <th>  No. Observations:  </th>  <td>   753</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                <td>Probit</td>      <th>  Df Residuals:      </th>  <td>   751</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     1</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 23 Feb 2025</td> <th>  Pseudo R-squ.:     </th>  <td>0.03860</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>17:14:57</td>     <th>  Log-Likelihood:    </th> <td> -495.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -514.87</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>2.882e-10</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>   -0.8502</td> <td>    0.172</td> <td>   -4.948</td> <td> 0.000</td> <td>   -1.187</td> <td>   -0.513</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>salario_ofrecido</th> <td>    0.2609</td> <td>    0.042</td> <td>    6.156</td> <td> 0.000</td> <td>    0.178</td> <td>    0.344</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       lfp        & \\textbf{  No. Observations:  } &      753    \\\\\n",
       "\\textbf{Model:}            &      Probit      & \\textbf{  Df Residuals:      } &      751    \\\\\n",
       "\\textbf{Method:}           &       MLE        & \\textbf{  Df Model:          } &        1    \\\\\n",
       "\\textbf{Date:}             & Sun, 23 Feb 2025 & \\textbf{  Pseudo R-squ.:     } &  0.03860    \\\\\n",
       "\\textbf{Time:}             &     17:14:57     & \\textbf{  Log-Likelihood:    } &   -495.00   \\\\\n",
       "\\textbf{converged:}        &       True       & \\textbf{  LL-Null:           } &   -514.87   \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{  LLR p-value:       } & 2.882e-10   \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                           & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}             &      -0.8502  &        0.172     &    -4.948  &         0.000        &       -1.187    &       -0.513     \\\\\n",
       "\\textbf{salario\\_ofrecido} &       0.2609  &        0.042     &     6.156  &         0.000        &        0.178    &        0.344     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Probit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          Probit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                    lfp   No. Observations:                  753\n",
       "Model:                         Probit   Df Residuals:                      751\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Sun, 23 Feb 2025   Pseudo R-squ.:                 0.03860\n",
       "Time:                        17:14:57   Log-Likelihood:                -495.00\n",
       "converged:                       True   LL-Null:                       -514.87\n",
       "Covariance Type:            nonrobust   LLR p-value:                 2.882e-10\n",
       "====================================================================================\n",
       "                       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const               -0.8502      0.172     -4.948      0.000      -1.187      -0.513\n",
       "salario_ofrecido     0.2609      0.042      6.156      0.000       0.178       0.344\n",
       "====================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "x = df['salario_ofrecido']  #Variable independiente sin constante.\n",
    "X = sm.add_constant(x)\n",
    "y = df['lfp']  #Variable dependiente.\n",
    "\n",
    "# Ajustar el modelo logístico\n",
    "probit_model = sm.Probit(y, X)\n",
    "result = probit_model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "\n",
    "\n",
    "class MyProbit(GenericLikelihoodModel):\n",
    "    def loglike(self, params):\n",
    "        exog = self.exog\n",
    "        endog = self.endog\n",
    "        q = 2 * endog - 1\n",
    "        return stats.norm.logcdf(q*np.dot(exog, params)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.657366\n",
      "         Iterations: 47\n",
      "         Function evaluations: 89\n",
      "                               MyProbit Results                               \n",
      "==============================================================================\n",
      "Dep. Variable:                    lfp   Log-Likelihood:                -495.00\n",
      "Model:                       MyProbit   AIC:                             994.0\n",
      "Method:            Maximum Likelihood   BIC:                             1003.\n",
      "Date:                Sun, 23 Feb 2025                                         \n",
      "Time:                        17:13:18                                         \n",
      "No. Observations:                 753                                         \n",
      "Df Residuals:                     751                                         \n",
      "Df Model:                           1                                         \n",
      "====================================================================================\n",
      "                       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "const               -0.8502      0.172     -4.948      0.000      -1.187      -0.513\n",
      "salario_ofrecido     0.2609      0.042      6.156      0.000       0.178       0.344\n",
      "====================================================================================\n"
     ]
    }
   ],
   "source": [
    "sm_probit_manual = MyProbit(y, X).fit()\n",
    "print(sm_probit_manual.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.657445\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>lfp</td>       <th>  No. Observations:  </th>  <td>   753</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   751</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     1</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 23 Feb 2025</td> <th>  Pseudo R-squ.:     </th>  <td>0.03849</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>17:15:12</td>     <th>  Log-Likelihood:    </th> <td> -495.06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -514.87</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>3.062e-10</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>   -1.3784</td> <td>    0.284</td> <td>   -4.853</td> <td> 0.000</td> <td>   -1.935</td> <td>   -0.822</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>salario_ofrecido</th> <td>    0.4225</td> <td>    0.071</td> <td>    5.986</td> <td> 0.000</td> <td>    0.284</td> <td>    0.561</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       lfp        & \\textbf{  No. Observations:  } &      753    \\\\\n",
       "\\textbf{Model:}            &      Logit       & \\textbf{  Df Residuals:      } &      751    \\\\\n",
       "\\textbf{Method:}           &       MLE        & \\textbf{  Df Model:          } &        1    \\\\\n",
       "\\textbf{Date:}             & Sun, 23 Feb 2025 & \\textbf{  Pseudo R-squ.:     } &  0.03849    \\\\\n",
       "\\textbf{Time:}             &     17:15:12     & \\textbf{  Log-Likelihood:    } &   -495.06   \\\\\n",
       "\\textbf{converged:}        &       True       & \\textbf{  LL-Null:           } &   -514.87   \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{  LLR p-value:       } & 3.062e-10   \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                           & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}             &      -1.3784  &        0.284     &    -4.853  &         0.000        &       -1.935    &       -0.822     \\\\\n",
       "\\textbf{salario\\_ofrecido} &       0.4225  &        0.071     &     5.986  &         0.000        &        0.284    &        0.561     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Logit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                    lfp   No. Observations:                  753\n",
       "Model:                          Logit   Df Residuals:                      751\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Sun, 23 Feb 2025   Pseudo R-squ.:                 0.03849\n",
       "Time:                        17:15:12   Log-Likelihood:                -495.06\n",
       "converged:                       True   LL-Null:                       -514.87\n",
       "Covariance Type:            nonrobust   LLR p-value:                 3.062e-10\n",
       "====================================================================================\n",
       "                       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const               -1.3784      0.284     -4.853      0.000      -1.935      -0.822\n",
       "salario_ofrecido     0.4225      0.071      5.986      0.000       0.284       0.561\n",
       "====================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajustar el modelo logístico\n",
    "logit_model = sm.Logit(y, X)\n",
    "result = logit_model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.657445\n",
      "         Iterations: 49\n",
      "         Function evaluations: 92\n",
      "                               MyLogit Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                    lfp   Log-Likelihood:                -495.06\n",
      "Model:                        MyLogit   AIC:                             994.1\n",
      "Method:            Maximum Likelihood   BIC:                             1003.\n",
      "Date:                Sun, 23 Feb 2025                                         \n",
      "Time:                        17:15:28                                         \n",
      "No. Observations:                 753                                         \n",
      "Df Residuals:                     751                                         \n",
      "Df Model:                           1                                         \n",
      "====================================================================================\n",
      "                       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "const               -1.3784      0.284     -4.853      0.000      -1.935      -0.822\n",
      "salario_ofrecido     0.4225      0.071      5.986      0.000       0.284       0.561\n",
      "====================================================================================\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "class MyLogit(GenericLikelihoodModel):\n",
    "    def loglike(self, params):\n",
    "        exog = self.exog\n",
    "        endog = self.endog\n",
    "        # Calcula el logit, que es el logaritmo de la probabilidad de éxito / la probabilidad de fracaso\n",
    "        linear_pred = np.dot(exog, params)\n",
    "        # Función de enlace logit: log(p / (1 - p))\n",
    "        p = 1 / (1 + np.exp(-linear_pred))\n",
    "        # Log-verosimilitud usando la distribución Bernoulli\n",
    "        return (endog * np.log(p) + (1 - endog) * np.log(1 - p)).sum()\n",
    "    \n",
    "sm_logit_manual = MyLogit(y, X).fit()\n",
    "print(sm_logit_manual.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
